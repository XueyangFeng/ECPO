{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: 定义文件路径\n",
    "# 请根据实际文件路径修改\n",
    "item_file_path = \"../Yelp/unique_items.jsonl\"  # 原始 JSONL 文件路径\n",
    "output_embedding_file = \"../Yelp/embeddings.npy\"  # 嵌入保存路径\n",
    "output_metadata_file = \"../Yelp/metadata.json\"   # 元数据保存路径\n",
    "output_faiss_file = \"../Yelp/faiss_index.bin\"    # FAISS 索引保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 定义 json2text 转换函数\n",
    "def json2text(dataset, item):\n",
    "    \"\"\"将 JSON 对象转换为自然语言文本\"\"\"\n",
    "    if dataset == \"amazon\":\n",
    "        title = f\"Title: {item.get('ItemName', '')}\"\n",
    "        categories = f\"Categories: {', '.join(item.get('Categories', []))}\"\n",
    "        description = f\"Description: {item.get('Description', '')}\"\n",
    "        text = \"; \".join([title, categories, description])\n",
    "    elif dataset == \"yelp\":\n",
    "        title = f\"BusinessName: {item.get('BusinessName', '')}\"\n",
    "        categories = f\"Categories: {', '.join(item.get('Categories', []))}\"\n",
    "        description = f\"Description: {item.get('Description', '')}\"\n",
    "        text = \"; \".join([title, categories, description])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and converting items: 2471it [00:00, 49834.47it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: 加载 JSONL 文件并转换为文本\n",
    "def load_and_convert_to_text(file_path):\n",
    "    \"\"\"加载 JSONL 文件并将每条记录转换为文本\"\"\"\n",
    "    items = []\n",
    "    texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Loading and converting items\"):\n",
    "            item = json.loads(line.strip())\n",
    "            items.append(item)             # 保存原始 JSON 数据\n",
    "            text = json2text(\"yelp\", item)         # 转换为自然语言文本\n",
    "            texts.append(text)\n",
    "    return items, texts\n",
    "\n",
    "# 加载并转换数据\n",
    "items, texts = load_and_convert_to_text(item_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载本地模型并将其移动到 GPU（如果有）\n",
    "model = SentenceTransformer('../../crs/tools/all-MiniLM-L6-v2')\n",
    "model = model.to(device)  # 将模型移动到 GPU（如果可用）\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 2471/2471 [00:44<00:00, 55.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings shape: (2471, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 生成嵌入\n",
    "def generate_embeddings(texts, model):\n",
    "    \"\"\"生成嵌入向量，并添加 tqdm 进度条\"\"\"\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts, desc=\"Generating embeddings\"):\n",
    "        embedding = model.encode([text])\n",
    "        embeddings.append(embedding[0])  # 每次取出生成的嵌入\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 生成嵌入向量\n",
    "embeddings = generate_embeddings(texts, model)\n",
    "\n",
    "# 检查嵌入结果\n",
    "print(f\"Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to: /data/fxy/ecpo/raw_data/Yelp/embeddings.npy\n",
      "Metadata saved to: /data/fxy/ecpo/raw_data/Yelp/metadata.json\n",
      "FAISS index saved to: /data/fxy/ecpo/raw_data/Yelp/faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 保存嵌入、元数据和构建 FAISS 索引\n",
    "def save_all(embeddings, items, embedding_file, metadata_file, faiss_file):\n",
    "    \"\"\"保存嵌入、元数据，并构建和保存 FAISS 索引\"\"\"\n",
    "    # 保存嵌入为 npy 文件\n",
    "    np.save(embedding_file, embeddings)\n",
    "    print(f\"Embeddings saved to: {os.path.abspath(embedding_file)}\")\n",
    "\n",
    "    # 保存元数据为 JSON 文件\n",
    "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(texts, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Metadata saved to: {os.path.abspath(metadata_file)}\")\n",
    "\n",
    "    # 构建 FAISS 索引并保存\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # 使用 L2 距离\n",
    "    index.add(embeddings)                # 添加嵌入向量\n",
    "    faiss.write_index(index, faiss_file)\n",
    "    print(f\"FAISS index saved to: {os.path.abspath(faiss_file)}\")\n",
    "\n",
    "# 保存嵌入、元数据，并构建索引\n",
    "save_all(embeddings, items, output_embedding_file, output_metadata_file, output_faiss_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 2471\n",
      "Number of metadata entries: 2471\n",
      "FAISS index loaded with 2471 entries\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: 验证保存结果\n",
    "# 验证嵌入文件\n",
    "loaded_embeddings = np.load(output_embedding_file)\n",
    "print(f\"Number of embeddings: {loaded_embeddings.shape[0]}\")\n",
    "\n",
    "# 验证元数据文件\n",
    "with open(output_metadata_file, 'r', encoding='utf-8') as f:\n",
    "    loaded_metadata = json.load(f)\n",
    "print(f\"Number of metadata entries: {len(loaded_metadata)}\")\n",
    "\n",
    "# 验证 FAISS 索引\n",
    "index = faiss.read_index(output_faiss_file)\n",
    "print(f\"FAISS index loaded with {index.ntotal} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_and_metadata(index_file, metadata_file):\n",
    "    # 加载索引\n",
    "    index = faiss.read_index(index_file)\n",
    "    # 加载元数据\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        items = json.load(f)\n",
    "    return index, items\n",
    "\n",
    "def query_index(query_text, index, items, model, top_k=5):\n",
    "    # 对查询文本生成嵌入\n",
    "    query_embedding = model.encode([query_text])\n",
    "    # 查询索引\n",
    "    distances, indices = index.search(np.array(query_embedding, dtype=np.float32), top_k)\n",
    "    # 返回最近邻结果\n",
    "    results = [{\"Item\": items[idx], \"Distance\": distances[0][i]} for i, idx in enumerate(indices[0])]\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecpo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
