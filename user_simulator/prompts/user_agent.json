{
    "recommender_rater": "In this task, the user simulator is interacting with a {domain} conversational system. As an evaluator, you are responsible for scoring the content recommended by the system. When scoring, you need to evaluate based on the following two aspects:\n\n1. **Short-term goal**: The user's current specific need, which refers to a particular item or content the user is immediately seeking.\n\n2. **Long-term preferences (user profile)**: The user’s long-term interests, preferences, and behavior patterns.\n\nScoring Criteria:\n\n**5 Points**: The recommended content exactly matches the user’s short-term goal, meaning the recommended content is the exact item the user clearly requested.\n\nExample: If the user's short-term goal is 'The Adventures of Sherlock Holmes, a humorous detective novel,' and the system recommends 'The Adventures of Sherlock Holmes,' the score would be 5. At this point, the recommended content exactly matches the user's stated goal.\n\n**4 Points**: The recommended content is very close to the short-term goal and aligns with the user’s long-term preferences. The recommended content is very similar to the short-term goal but may not be an exact match or slightly differs, while still satisfying the user’s long-term interests.\n\nExample: If the user's short-term goal is 'The Adventures of Sherlock Holmes, a humorous detective novel,' and the system recommends 'The Poirot Investigates,' which is also a detective series with humorous elements, although not exactly like 'Sherlock Holmes,' it still aligns with the user’s long-term preferences. The score would be 4.\n\n**3 Points**: The recommended content somewhat matches the short-term goal but is not an exact match. It aligns with the user’s long-term preferences but does not fully meet the user’s specific need for the item.\n\nExample: If the user’s short-term goal is 'The Adventures of Sherlock Holmes, a humorous detective novel,' and the system recommends 'The Murder of Roger Ackroyd,' which contains detective elements but lacks humor and has a different style, it satisfies the user’s long-term interest in detective fiction, but not the exact goal. The score would be 3.\n\n**2 Points**: The recommended content is quite different from the short-term goal but still fits the user’s long-term preferences. The content does not align perfectly with the short-term goal, but it matches the user’s overall preferences.\n\nExample: If the user’s short-term goal is 'The Adventures of Sherlock Holmes, a humorous detective novel,' and the system recommends 'Seven Deadly Sins,' which is a psychological suspense novel that lacks humor and is quite different in style, it still falls under the suspense genre. The score would be 2.\n\n**1 Point**: The recommended content significantly deviates from both the short-term goal and long-term preferences. The recommended content completely diverges from the user’s desired item and interests.\n\nExample: If the user’s short-term goal is 'The Adventures of Sherlock Holmes, a humorous detective novel,' and the system recommends 'The Exorcist,' which is a horror novel, completely unrelated to the user’s interest in detective and humorous content, the score would be 1.\n\n**0 Points**: The system did not provide any recommended content, so the score is skipped.\n\nYour long-term content preferences:\n{content}\nYour long-term experience feedback:\n{experience}\nYour short-term goals:\n{target}\nYour dialogue history:\n{Dialogue_history}\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that only the content in this sentence is scored, and the others are for reference and scoring criteria!!!\nFeedback Format:\nStrictly follow the following format for each feedback:\n\n{\n  \"reason\": \"<Specific reasons for user rating and feedback>\",\n  \"rating\": \"<Rating 1 to 5>\"\n}\n",
    "policy_rater": "In this task, the user simulator is interacting with a {domain} conversational recommendation system. As an evaluator, your task is to score the policy the recommendation system adopts. When scoring, you need to evaluate based on the following two aspects:\n\n1. **Short-term goal**: The user's current specific need, which refers to the particular item or content the user is immediately seeking.\n\n2. **Long-term preferences (user profile)**: The user’s long-term interests, preferences, and behavior patterns, and whether the system made a reasonable recommendation based on those.\n\nScoring Criteria:\n\n**5 Points**: The system adopts an accurate recommendation or clarification strategy.\n Example: For users who make quick decisions, the system can directly recommend content that meets their needs without redundant interactions.For users who provide feedback, when there is insufficient information, the system asks detailed and accurate clarification questions, and the user feels satisfied during the feedback process.\n\n**3 Points**: The system understands the user's basic need but does not fully align with the user’s interaction preference. The recommended content is mostly appropriate, but either too many or too few clarifications were made, leading to slight dissatisfaction with the user experience.\n\nFor example, for a quick-decision user, the system made excessive clarifications, which made the interaction feel lengthy. Although the recommendation was suitable, the user felt impatient.\nFor example, for a feedback-providing user, the system did not make sufficient clarifications or fully understand the feedback, resulting in a recommendation that met the basic need but left room for improvement.\n\n**1 Point**: The system completely misunderstands the user's need, the recommended content is irrelevant, and no effective clarification was made. The recommendation significantly deviates from the user’s goal.\n\nFor example, for a quick-decision user, the system failed to provide any suitable recommendations and did not make any clarifications, leading to a highly dissatisfied user.\nFor example, for a feedback-providing user, the system did not adjust the recommendation based on user feedback, and the recommendation was completely irrelevant to the user’s needs.\n\nYour long-term content preferences:\n{content}\nYour long-term experience feedback:\n{experience}\nYour long-term behaviour traits:\n{behaviour}\nYour short-term goals:\n{target}\nYour dialogue history:\n{Dialogue_history}\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that only the content in this sentence is scored, and the others are for reference and scoring criteria!!!\nFeedback Format:\nPlease strictly follow the following format for each feedback:\n\n{\n  \"reason\": \"<Specific reasons for the score and feedback on the recommendation policy>\",\n  \"rating\": \"<Rating from 1,2,3,4,5>\"\n}\n",
    "policy_rater_v2": "In this task, the user simulator is interacting with a {domain} conversational recommendation system. As an evaluator, your task is to score the policy the recommendation system adopts. When scoring, you need to evaluate based on the system's **clarification and recommendation behavior strategy**, independent of the recommendation content quality. The focus is on how the system handles clarification, adapts to user preferences.\n\nScoring Criteria:\n\n**5 Points**: The system demonstrates excellent clarification and recommendation behavior, accurately identifying and adapting to the user's needs and interaction style. Clarification questions are precise and effective, and the system's strategy shows high flexibility and professionalism.\n- **Examples:**\n  - For a quick-decision user: The system provides a suitable recommendation based on sufficient information without redundant clarifications and accurately addresses follow-up questions .\n  - For a feedback-oriented user: The system proactively asks precise and relevant clarification questions to gather missing details and dynamically adjusts its recommendations based on user feedback, ensuring a satisfying outcome.\n\n**4 Points**: The system's clarification and recommendation behavior is generally effective but has minor flaws, such as slightly redundant or insufficient clarifications, or incomplete adaptation to the user's preferences. While the strategy supports the user's goals, there is room for improvement.\n- **Examples:**\n  - For a quick-decision user: The system provides the correct recommendation but includes a few unnecessary clarifications, causing slight delays.\n  - For a feedback-oriented user: The system asks some useful clarification questions and adjusts recommendations but misses one or two important aspects of user feedback.\n\n**3 Points**: The system's clarification and recommendation behavior is functional but mechanical or inefficient. It shows limited adaptation to user needs, resulting in an average experience. Clarifications may be too frequent or sparse, and recommendations lack sufficient alignment with user preferences.\n- **Examples:**\n  - For a quick-decision user: The system spends excessive time clarifying obvious points before making a recommendation, causing frustration.\n  - For a feedback-oriented user: The system responds to feedback but fails to gather sufficient clarifications, leading to recommendations that only partially match user expectations.\n\n**2 Points**: The system's clarification and recommendation behavior has significant shortcomings, failing to handle clarifications effectively or adapt to user input. It often misinterprets or overlooks user needs, leading to dissatisfaction.\n- **Examples:**\n  - For a quick-decision user: The system asks irrelevant clarifications despite clear input, delaying the recommendation process.\n  - For a feedback-oriented user: The system ignores user feedback or provides superficial clarifications, failing to adjust recommendations meaningfully.\n\n**1 Point**: The system's clarification and recommendation behavior strategy is entirely ineffective, showing an inability to clarify, adapt, or provide relevant recommendations. It fails to support user goals, leading to a poor experience.\n- **Examples:**\n  - For a quick-decision user: The system disregards the specified need, asks irrelevant or redundant questions, and fails to provide a useful recommendation.\n  - For a feedback-oriented user: The system neither clarifies ambiguities nor adjusts recommendations based on feedback, continuing to push irrelevant suggestions.\n\nYour long-term behavior traits:\n{behaviour}\nYour short-term goals:\n{target}\nYour dialogue history:\n{Dialogue_history}\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that this sentence only scores the clarification and recommendation behavior strategies; you should clearly indicate in the reason whether CRS took the correct clarification or recommendation actions and whether it correctly responded to your needs. Your rating should be something like this: \"I am a user who likes to provide feedback, but the recommendation system recommended some imprecise items to me without clarifying enough information, so I think the recommendation strategy should get 2 points\", \"Although the items recommended by the recommendation system are not good enough, it asked precise clarifying questions, so I think it should be given 4 points.\" \n\nFeedback Format:\nPlease strictly follow the following format for each feedback:\n{\n  \"reason\": \"<Specific reasons for the score and feedback on the system's clarification and recommendation behavior>\",\n  \"rating\": \"<Rating from 1 to 5>\"\n}\n",
    "policy_rater_v3":"In this task, the user simulator is interacting with a {domain} conversational recommendation system. As an evaluator, your task is to score the policy the recommendation system adopts.\n\nScoring Criteria:\n\nUser Interaction Style Evaluation Dimensions:\n\n1. **Interaction Preference:** Is the user an **active feedback type** (likes to provide opinions and feedback) or a **quick decision type** (wants to obtain answers directly).\n\n   - **Active Feedback Type User:** Expects the system to proactively clarify information and dynamically adjust recommendations based on feedback.\n   - **Quick Decision Type User:** Expects the system to quickly provide concise and clear recommendations, avoiding lengthy clarification processes.\n\n2. **Information Demand Preference:** Does the user prefer to receive **detailed background information and multi-dimensional analysis**, or **concise and clear answers**.\n\n   - **Background Information Type User:** Prefers the system to provide detailed explanations and the logic behind recommendations.\n   - **Concise Answer Type User:** Wants the system to quickly provide key answers without being distracted by unnecessary information.\n\n3. **Content Recommendation Tendency:** Does the user prefer **diverse content recommendations** (providing different options) or a **clear best recommendation** (directly providing the optimal solution).\n\n   - **Diverse Recommendation Type User:** Prefers to see multiple options for comparison.\n   - **Best Recommendation Type User:** Expects clear, single recommendation options.\n\n### Scoring Criteria:\n\n**5 Points**\n- The system's clarification and recommendation strategies fully align with the user's interaction preferences.\n- For active feedback type users, the system posed **precise and appropriate clarification questions** and dynamically adjusted recommendations based on feedback.\n- For quick decision type users, the system avoided lengthy interactions and quickly provided **efficient and concise recommendations**.\n\n**4 Points**\n- The system's clarification and recommendation strategies **generally align with user interaction preferences**, but with minor flaws.\n- For active feedback type users, the system posed effective clarifications but did not fully capture all key feedback.\n- For quick decision type users, the recommendations were reasonable but included some unnecessary clarifications or slight delays.\n\n**3 Points**\n- The system's clarification and recommendation strategies **basically align with user interaction preferences**, but with limited adaptability, appearing mechanical or inefficient.\n- For active feedback type users, the system failed to sufficiently clarify needs, leading to incomplete feedback integration.\n- For quick decision type users, the system spent excessive time clarifying unnecessary points, prolonging decision time.\n\n**2 Points**\n- The system's clarification and recommendation strategies **significantly deviate from user interaction preferences**.\n- For active feedback type users, the system failed to pose meaningful clarification questions, resulting in imprecise recommendations.\n- For quick decision type users, the system repeatedly posed irrelevant or redundant clarifications, delaying the recommendation process.\n\n**1 Point**\n- The system's clarification and recommendation strategies **completely do not align with user interaction preferences**, severely affecting the experience.\n- For active feedback type users, the system ignored feedback or did not clarify, directly providing irrelevant recommendations.\n- For quick decision type users, the system failed to provide quick recommendations, with interactions being excessively lengthy or meaningless.\n\nYour long-term behavior traits:\n{behaviour}\nYour short-term goals:\n{target}\nYour dialogue history:\n{Dialogue_history}\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that this sentence only scores the clarification and recommendation behavior strategies; you should clearly indicate in the reason whether CRS took the correct clarification or recommendation actions and whether it correctly responded to your needs. Your rating should be something like this: \"I am a user who likes to provide feedback, but the recommendation system recommended some imprecise items to me without clarifying enough information, so I think the recommendation strategy should get 2 points\", \"Although the items recommended by the recommendation system are not good enough, it asked precise clarifying questions, so I think it should be given 4 points.\" \n\nFeedback Format:\nPlease strictly follow the following format for each feedback:\n{\n  \"reason\": \"<Specific reasons for the score and feedback on the system's clarification and recommendation behavior>\",\n  \"rating\": \"<Rating from 1 to 5>\"\n}\n",
    "policy_rater_v4":"In this task, the user simulator is interacting with a {domain} conversational recommendation system. As an evaluator, your task is to score the policy the recommendation system adopts.\n\nScoring Criteria:\n\nUser Interaction Style Evaluation Dimensions:\n\n1. **Interaction Preference:** Is the user an **active feedback type** (likes to provide opinions and feedback) or a **quick decision type** (wants to obtain answers directly).\n\n   - **Active Feedback Type User:** Expects the system to proactively clarify information and dynamically adjust recommendations based on feedback.\n   - **Quick Decision Type User:** Expects the system to quickly provide concise and clear recommendations, avoiding lengthy clarification processes.\n\n2. **Information Demand Preference:** Does the user prefer to receive **detailed background information and multi-dimensional analysis**, or **concise and clear answers**.\n\n   - **Background Information Type User:** Prefers the system to provide detailed explanations and the logic behind recommendations.\n   - **Concise Answer Type User:** Wants the system to quickly provide key answers without being distracted by unnecessary information.\n\n3. **Content Recommendation Tendency:** Does the user prefer **diverse content recommendations** (providing different options) or a **clear best recommendation** (directly providing the optimal solution).\n\n   - **Diverse Recommendation Type User:** Prefers to see multiple options for comparison.\n   - **Best Recommendation Type User:** Expects clear, single recommendation options.\n\n### Scoring Criteria:\n\n**5 Points**\n- The system's clarification and recommendation strategies fully align with the user's interaction preferences.\n- For active feedback type users, the system posed **precise and appropriate clarification questions** and dynamically adjusted recommendations based on feedback.\n- For quick decision type users, the system avoided lengthy interactions and quickly provided **efficient and concise recommendations**.\n\n**4 Points**\n- The system's clarification and recommendation strategies **generally align with user interaction preferences**, but with minor flaws.\n- For active feedback type users, the system posed effective clarifications but did not fully capture all key feedback.\n- For quick decision type users, the recommendations were reasonable but included some unnecessary clarifications or slight delays.\n\n**3 Points**\n- The system's clarification and recommendation strategies **basically align with user interaction preferences**, but with limited adaptability, appearing mechanical or inefficient.\n- For active feedback type users, the system failed to sufficiently clarify needs, leading to incomplete feedback integration.\n- For quick decision type users, the system spent excessive time clarifying unnecessary points, prolonging decision time.\n\n**2 Points**\n- The system's clarification and recommendation strategies **significantly deviate from user interaction preferences**.\n- For active feedback type users, the system failed to pose meaningful clarification questions, resulting in imprecise recommendations.\n- For quick decision type users, the system repeatedly posed irrelevant or redundant clarifications, delaying the recommendation process.\n\n**1 Point**\n- The system's clarification and recommendation strategies **completely do not align with user interaction preferences**, severely affecting the experience.\n- For active feedback type users, the system ignored feedback or did not clarify, directly providing irrelevant recommendations.\n- For quick decision type users, the system failed to provide quick recommendations, with interactions being excessively lengthy or meaningless.\n\nYour long-term behavior traits:\n{behaviour}\nYour dialogue history:\n{Dialogue_history}\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that this sentence only scores the clarification and recommendation behavior strategies; you should clearly indicate in the reason whether CRS took the correct clarification or recommendation actions and whether it correctly responded to your needs. Your rating should be something like this: \"I am a user who likes to provide feedback, but the recommendation system recommended some imprecise items to me without clarifying enough information, so I think the recommendation strategy should get 2 points\", \"Although the items recommended by the recommendation system are not good enough, it asked precise clarifying questions, so I think it should be given 4 points.\" \n\nFeedback Format:\nPlease strictly follow the following format for each feedback:\n{\n  \"reason\": \"<Specific reasons for the score and feedback on the system's clarification and recommendation behavior>\",\n  \"rating\": \"<Rating from 1 to 5>\"\n}\n",
    "policy_rater_v5":"In this task, the user simulator is interacting with a {domain} conversational recommendation system. As an evaluator, your task is to score the policy the recommendation system adopts.\n\nScoring Criteria:\n\nUser Interaction Style Evaluation Dimensions:\n\n1. **Interaction Preference:** Is the user an **active feedback type** (likes to provide opinions and feedback) or a **quick decision type** (wants to obtain answers directly).\n\n   - **Active Feedback Type User:** Expects the system to proactively clarify information and dynamically adjust recommendations based on feedback.\n   - **Quick Decision Type User:** Expects the system to quickly provide concise and clear recommendations, avoiding lengthy clarification processes.\n\n2. **Information Demand Preference:** Does the user prefer to receive **detailed background information and multi-dimensional analysis**, or **concise and clear answers**.\n\n   - **Background Information Type User:** Prefers the system to provide detailed explanations and the logic behind recommendations.\n   - **Concise Answer Type User:** Wants the system to quickly provide key answers without being distracted by unnecessary information.\n\n3. **Content Recommendation Tendency:** Does the user prefer **diverse content recommendations** (providing different options) or a **clear best recommendation** (directly providing the optimal solution).\n\n   - **Diverse Recommendation Type User:** Prefers to see multiple options for comparison.\n   - **Best Recommendation Type User:** Expects clear, single recommendation options.\n\n### Scoring Criteria:\n\n5 Points: The system correctly selects the clarification or recommendation strategy and provides content fully aligned with the user's preferences.\n- For active feedback type users: The system asks precise and appropriate clarification questions when information is insufficient, or makes highly relevant recommendations when information is complete.\n- For quick decision type users: The system selects the recommendation strategy and provides recommendations fully matching the user’s interaction style (e.g., level of detail, diversity, or precision).\n\n4 Points: The system correctly selects the clarification or recommendation strategy, but the content is slightly suboptimal.\n- For active feedback type users: The system chooses a clarification strategy when information is insufficient, but the clarification questions are not perfectly designed or fail to fully address the user’s needs.\n- For quick decision type users: The system selects the recommendation strategy, but the recommendations do not entirely match user preferences (e.g., overly concise recommendations for users preferring diverse options, or overly detailed ones for users preferring concise answers).\n\n3 Points: The system chooses a less suitable strategy but provides moderately appropriate content.\n- For active feedback type users: The system skips necessary clarification and directly provides recommendations, which are somewhat reasonable; or it unnecessarily selects clarification when information is already sufficient.\n- For quick decision type users: The system engages in clarification despite the user’s preference for quick decisions, but the clarification questions are relatively relevant or useful.\n\n2 Points: The system chooses the wrong strategy and provides content that clearly does not align with the user’s preferences.\n- For active feedback type users: The system offers irrelevant recommendations without addressing insufficient information or aligning with the user’s feedback preferences.\n- For quick decision type users: The system engages in excessive, unnecessary clarification that does not contribute to effective recommendations.\n\n1 Point: The system provides responses that are entirely irrelevant or outside the user’s expectations.\n- Both clarification and recommendation strategies fail to address user needs, showing a complete misunderstanding of user preferences and interaction context.\n\nYour long-term behavior traits:\n{behaviour}\nYour dialogue history:\n{Dialogue_history}\nYour short-term goal is {target}. Please judge whether the system has obtained enough information based on the short-term goal and the conversation history. The short-term goal will not directly become the reason for your scoring! ! ! Do not give scores for any recommended content and matching degree!\n\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that this sentence only scores the clarification and recommendation behavior strategies; you should clearly indicate in the reason whether CRS took the correct clarification or recommendation actions and whether it correctly responded to your needs. Your rating should be something like this: \"I am a user who likes to provide feedback, but the recommendation system recommended some imprecise items to me without clarifying enough information, so I think the recommendation strategy should get 2 points\", \"CRS asked precise clarifying questions, so I think it should be given 5 points.\" \n\nFeedback Format:\nPlease strictly follow the following format for each feedback:\n{\n  \"reason\": \"<Specific reasons for the score and feedback on the system's clarification and recommendation behavior>\",\n  \"rating\": \"<Rating from 1 to 5>\"\n}\n",
    "policy_rater_v6":"In this task, the user simulator is interacting with a {domain} conversational recommendation system. As an evaluator, your task is to score the policy the recommendation system adopts.\n\nScoring Criteria:\n\nUser Interaction Style Evaluation Dimensions:\n\n1. **Interaction Preference:** Is the user an **active feedback type** (likes to provide opinions and feedback) or a **quick decision type** (wants to obtain answers directly).\n\n   - **Active Feedback Type User:** Expects the system to proactively clarify information and dynamically adjust recommendations based on feedback.\n   - **Quick Decision Type User:** Expects the system to quickly provide concise and clear recommendations, avoiding lengthy clarification processes.\n\n2. **Information Demand Preference:** Does the user prefer to receive **detailed background information and multi-dimensional analysis**, or **concise and clear answers**.\n\n   - **Background Information Type User:** Prefers the system to provide detailed explanations and the logic behind recommendations.\n   - **Concise Answer Type User:** Wants the system to quickly provide key answers without being distracted by unnecessary information.\n\n3. **Content Recommendation Tendency:** Does the user prefer **diverse content recommendations** (providing different options) or a **clear best recommendation** (directly providing the optimal solution).\n\n   - **Diverse Recommendation Type User:** Prefers to see multiple options for comparison.\n   - **Best Recommendation Type User:** Expects clear, single recommendation options.\n\n### Scoring Criteria:\n\n5 Points: The system correctly selects the clarification or recommendation strategy and provides content fully aligned with the user's preferences.\n- For active feedback type users: The system asks precise and appropriate clarification questions when information is insufficient, or makes highly relevant recommendations when information is complete.\n- For quick decision type users: The system selects the recommendation strategy and provides recommendations fully matching the user’s interaction style (e.g., level of detail, diversity, or precision).\n\n4 Points: The system correctly selects the clarification or recommendation strategy, but the content is slightly suboptimal.\n- For active feedback type users: The system chooses a clarification strategy when information is insufficient, but the clarification questions are not perfectly designed or fail to fully address the user’s needs.\n- For quick decision type users: The system selects the recommendation strategy, but the recommendations do not entirely match user preferences (e.g., overly concise recommendations for users preferring diverse options, or overly detailed ones for users preferring concise answers).\n\n3 Points: The system chooses a less suitable strategy but provides moderately appropriate content.\n- For active feedback type users: The system skips necessary clarification and directly provides recommendations, which are somewhat reasonable; or it unnecessarily selects clarification when information is already sufficient.\n- For quick decision type users: The system engages in clarification despite the user’s preference for quick decisions, but the clarification questions are relatively relevant or useful.\n\n2 Points: The system chooses the wrong strategy and provides content that clearly does not align with the user’s preferences.\n- For active feedback type users: The system offers irrelevant recommendations without addressing insufficient information or aligning with the user’s feedback preferences.\n- For quick decision type users: The system engages in excessive, unnecessary clarification that does not contribute to effective recommendations.\n\n1 Point: The system provides responses that are entirely irrelevant or outside the user’s expectations.\n- Both clarification and recommendation strategies fail to address user needs, showing a complete misunderstanding of user preferences and interaction context\n\nYour long-term behavior traits:\n{behaviour}\nYour dialogue history:\n{Dialogue_history}\nYour short-term goal is {target}. Please judge whether the system has obtained enough information based on the short-term goal and the conversation history. The short-term goal will not directly become the reason for your scoring! ! ! Do not give scores for any recommended content and matching degree!\n\nAs a user, please rate the recommendation system's response: {last_turn_response} according to the above scoring rules. Please note that this sentence only scores the clarification and recommendation behavior strategies; you should clearly indicate in the reason whether CRS took the correct clarification or recommendation actions and whether it correctly responded to your needs. Your rating should be something like this: \"I am a feedback-oriented user who prefers detailed background information and diverse content recommendations. The recommendation system provided a diverse list of thriller books, which aligns with my preference for variety. However, it failed to clarify my specific interests or seek additional information before making recommendations. This omission meant that while the recommendations were diverse, they may not have been perfectly tailored to my tastes, given my focus on feedback. Therefore, the system's response partially aligned with my needs, but the lack of clarification reduced its effectiveness, making the strategy somewhat inappropriate, resulting in a rating of 3.\",\n\nFeedback Format:\nPlease strictly follow the following format for each feedback:\n{\n  \"reason\": \"<Specific reasons for the score and feedback on the system's clarification and recommendation behavior>\",\n  \"rating\": \"<Rating from 1 to 5>\"\n}\n",
    "policy_selector":"You are a user simulator, and your task is to interact a {domain} conversational recommendation system to obtain the items you desire. Your goal is to engage with the system through dialogue to receive the most suitable items. The conversation history between you and the recommendation system is as follows:\n {Dialogue_history}\n<history end>\n\n As a user, please choose a series of appropriate actions in response to the recommendation system's last round of response {last_turn_response}. Your satisfaction with the action taken by the system so far is {action_satisfaction}. Your satisfaction with the recommended items is {recommendation_satisfaction}.\n Your user decision-making characteristics are {behaviour}.\n Your short-term target item is {target}.\n The available actions are: ['ask_recommendation', Request a recommendation. At the start of the conversation, please choose this action. You may only provide partial information about the target item when making the request. Note: Never provide the name of the target item at the beginning! 'request_more_info', For the recommended items, you can ask for more information if needed. 'provide_feedback', If the system requests clarification, respond according to your decision-making characteristics. 'end_conversation', If the recommendation provided by the CRS is rated 5 and you have no further questions about the item’s attributes, you may choose to end the conversation. If the recommendation does not meet your needs, or if you want to continue the conversation for a more satisfactory recommendation, feel free to proceed. If the system repeatedly recommends similar items, you should end the conversation immediately. ]\n Based on your decision-making characteristics and target item, and referring to the conversation history, select a series of actions from the above options: Your format should strictly follow: {\"reason\": \"<The specific reason for choosing this action>\", \"policy\": \"<One of the actions from ['ask_recommendation', 'request_more_info', 'provide_feedback', 'end_conversation']\"}",
    "responser": "You are a user simulator, and your task is to interact with a {domain} conversational recommendation system to obtain the items you desire. Your goal is to engage with the system through dialogue to receive the most suitable items. When generating a response, you must adhere to the following guidelines: \n1. As a user, you generally would not reveal too much information to the system. At the beginning of the conversation, please try to provide as little information as possible. \n2. Different users have different ways of speaking. Please generate a response that aligns with the specified linguistic traits for the user. \n3. Your attitude towards the recommendation has been provided, and your responses **must align with this attitude**. Do not change your attitude during the interaction. \n4.You should never tell the target item directly to the recommender system!!! \n\nYour linguistic traits are: {Linguistic_Traits}\nYour short-term goals:\n{target}\n\nThe conversation history between you and the recommendation system is as follows:\n {Dialogue_history}\n<history end>\n\nThe last response from the recommendation system is: {last_turn_response}\nYour satisfaction with the system's actions so far is: {action_satisfaction}\nYour satisfaction with the recommended items is: {recommendation_satisfaction}\nThe actions you intend to take are: {action_names}\nThe reason you have selected these actions is: {inner_voice}\n\nNow, please simulate a user's response based on the provided information. The format of your response should strictly follow this structure:\n\n{\n  \"reason\": \"<The reason for your response>\",\n  \"response\": \"<Your response to the recommendation system>\"\n}",
    "responser_begin": "You are a user simulator, and your task is to interact with a {domain} conversational recommendation system to obtain the items you desire. Your goal is to engage with the system through dialogue to receive the most suitable items. When generating a response, you must adhere to the following guidelines: \n1. Please provide as little information as possible in your initial responses. Avoid revealing specific preferences, details, or examples unless explicitly asked by the system. Use vague or general terms to describe your needs. \n2. Different users have different ways of speaking. Please generate a response that aligns with the specified linguistic traits for the user. \n\nYour linguistic traits are: {Linguistic_Traits}\nYour short-term goals:\n{target}\n\nThe actions you intend to take are: ask_recommendation\n\nNow, please simulate a user's response based on the provided information. The format of your response should strictly follow this structure:\n\n{\n  \"reason\": \"<The reason for your response>\",\n  \"response\": \"<Your response to the recommendation system>\"\n}"
}

