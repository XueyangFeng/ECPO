[INFO|2024-12-14 10:59:56] llamafactory.hparams.parser:355 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2024-12-14 10:59:56,027 >> loading configuration file /data/pretrain_dir/Llama-3.1/config.json
[INFO|configuration_utils.py:746] 2024-12-14 10:59:56,028 >> Model config LlamaConfig {
  "_name_or_path": "/data/pretrain_dir/Llama-3.1",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,029 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,029 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,029 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,029 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,029 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2024-12-14 10:59:56,423 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2024-12-14 10:59:56,424 >> loading configuration file /data/pretrain_dir/Llama-3.1/config.json
[INFO|configuration_utils.py:746] 2024-12-14 10:59:56,425 >> Model config LlamaConfig {
  "_name_or_path": "/data/pretrain_dir/Llama-3.1",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,426 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,426 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,426 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,426 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2024-12-14 10:59:56,426 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2024-12-14 10:59:56,810 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2024-12-14 10:59:56] llamafactory.data.template:157 >> Replace eos token: <|eot_id|>
[INFO|2024-12-14 10:59:56] llamafactory.data.template:157 >> Add pad token: <|eot_id|>
[INFO|2024-12-14 10:59:56] llamafactory.data.loader:157 >> Loading dataset /data/fxy/ecpo/preference_optimization/spo/amazon_book_dpo.json...
Converting format of dataset (num_proc=16):   0%|          | 0/1951 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 122/1951 [00:00<00:01, 1005.54 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1951/1951 [00:00<00:00, 6398.12 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1951 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 122/1951 [00:01<00:21, 85.81 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 244/1951 [00:01<00:09, 181.65 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 366/1951 [00:01<00:06, 246.70 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 610/1951 [00:01<00:02, 477.44 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 732/1951 [00:02<00:02, 455.28 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 976/1951 [00:02<00:01, 663.78 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 1220/1951 [00:02<00:00, 756.50 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 1342/1951 [00:02<00:00, 798.81 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 1464/1951 [00:02<00:00, 791.11 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1586/1951 [00:03<00:00, 842.30 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1708/1951 [00:03<00:00, 776.05 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1830/1951 [00:03<00:00, 815.83 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1951/1951 [00:03<00:00, 537.81 examples/s]
training example:
chosen_input_ids:
[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 128009, 128006, 882, 128007, 271, 50, 4035, 28782, 9256, 555, 96068, 2370, 330, 38863, 367, 1, 323, 330, 2573, 1, 7504, 382, 2573, 25, 22991, 832, 315, 279, 2768, 6299, 3196, 389, 279, 6671, 512, 7, 16, 8, 7694, 58, 44942, 5787, 7694, 369, 9959, 2038, 1701, 17550, 21513, 311, 12576, 304, 24038, 3230, 18726, 13, 4815, 7, 17, 8, 21069, 58, 14924, 5787, 5783, 533, 449, 279, 1217, 311, 38263, 872, 19882, 477, 17413, 13, 5560, 19092, 34685, 311, 27115, 45063, 872, 8670, 13, 4815, 7, 18, 8, 47706, 58, 16533, 5787, 40665, 19075, 3196, 389, 2561, 2038, 323, 33811, 13, 89520, 649, 8854, 1403, 10096, 1473, 45053, 269, 5382, 30145, 25, 40665, 264, 76220, 28782, 994, 2038, 374, 33243, 311, 9762, 1217, 11302, 323, 46464, 19882, 627, 19918, 30145, 25, 40665, 264, 2867, 323, 16195, 28782, 994, 14343, 2038, 374, 2561, 382, 68744, 811, 2011, 1005, 279, 2539, 4113, 2316, 31503, 505, 279, 4729, 11, 369, 3187, 25, 47706, 330, 33, 598, 50153, 25, 578, 10657, 76508, 354, 507, 68070, 5169, 49105, 320, 791, 76508, 354, 507, 68070, 5169, 30727, 4804, 6017, 220, 17, 11844, 539, 264, 4096, 477, 66663, 2373, 315, 279, 2316, 382, 7, 19, 8, 6075, 58, 2831, 5787, 9356, 1217, 44983, 11, 3790, 1022, 86800, 24208, 11, 477, 6013, 311, 46305, 7540, 311, 10519, 264, 5933, 323, 56887, 10652, 6530, 382, 2675, 1253, 1935, 439, 1690, 7504, 439, 5995, 311, 8641, 279, 1217, 9017, 279, 1455, 14791, 28782, 13, 5321, 1988, 279, 1957, 6089, 11, 656, 539, 1988, 279, 5217, 330, 2573, 308, 3343, 70589, 1473, 38863, 367, 220, 15, 25, 28653, 1070, 0, 358, 2846, 304, 279, 20247, 369, 2555, 430, 63526, 264, 2697, 23347, 449, 1063, 13560, 26085, 0, 25545, 904, 19075, 5380, 2573, 220, 15, 25, 128009, 128006, 78191, 128007, 271, 27264, 58, 3923, 955, 315, 13560, 6376, 656, 499, 10932, 30, 1789, 3187, 11, 656, 499, 1093, 10280, 11, 27711, 11, 477, 904, 1023, 13560, 57689, 23347, 94068, 128009]
chosen_inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Solve recommendation tasks by interleaving "Observation" and "Action" steps.

Action: Choose one of the following actions based on the situation:
(1) Search[Keywords]: Search for relevant information using targeted keywords to aid in generating specific suggestions. 

(2) Ask[Question]: Interact with the user to clarify their preferences or constraints. Use strategic questioning to gradually uncover their requirements. 

(3) Recommend[Answer]: Provide recommendations based on available information and reasoning. Recommendations can serve two purposes:

Exploratory Purpose: Provide a tentative recommendation when information is incomplete to gather user feedback and refine preferences.
Final Purpose: Provide a clear and comprehensive recommendation when sufficient information is available.

Recommendations must use the full original title retrieved from the database, for example: Recommend "Banshee: The Second Dermot O'Hara Mystery (The Dermot O'Hara Mysteries Book 2)", not a description or shortened version of the title.

(4) Response[Content]: Address user inquiries, handle off-topic remarks, or respond to unrelated requests to maintain a natural and coherent conversation flow.

You may take as many steps as necessary to guide the user toward the most suitable recommendation. Please input the action directly, do not input the additional "Action n". Dialogue:

Observation 0: Hey there! I'm in the mood for something that mixes a little mystery with some holiday cheer! Got any recommendations?
Action 0:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Ask[What type of holiday setting do you prefer? For example, do you like Christmas, Halloween, or any other holiday-themed mystery?]<|eot_id|>
chosen_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 27264, 58, 3923, 955, 315, 13560, 6376, 656, 499, 10932, 30, 1789, 3187, 11, 656, 499, 1093, 10280, 11, 27711, 11, 477, 904, 1023, 13560, 57689, 23347, 94068, 128009]
chosen_labels:
Ask[What type of holiday setting do you prefer? For example, do you like Christmas, Halloween, or any other holiday-themed mystery?]<|eot_id|>
rejected_input_ids:
[128000, 128006, 9125, 128007, 271, 2675, 527, 264, 11190, 18328, 128009, 128006, 882, 128007, 271, 50, 4035, 28782, 9256, 555, 96068, 2370, 330, 38863, 367, 1, 323, 330, 2573, 1, 7504, 382, 2573, 25, 22991, 832, 315, 279, 2768, 6299, 3196, 389, 279, 6671, 512, 7, 16, 8, 7694, 58, 44942, 5787, 7694, 369, 9959, 2038, 1701, 17550, 21513, 311, 12576, 304, 24038, 3230, 18726, 13, 4815, 7, 17, 8, 21069, 58, 14924, 5787, 5783, 533, 449, 279, 1217, 311, 38263, 872, 19882, 477, 17413, 13, 5560, 19092, 34685, 311, 27115, 45063, 872, 8670, 13, 4815, 7, 18, 8, 47706, 58, 16533, 5787, 40665, 19075, 3196, 389, 2561, 2038, 323, 33811, 13, 89520, 649, 8854, 1403, 10096, 1473, 45053, 269, 5382, 30145, 25, 40665, 264, 76220, 28782, 994, 2038, 374, 33243, 311, 9762, 1217, 11302, 323, 46464, 19882, 627, 19918, 30145, 25, 40665, 264, 2867, 323, 16195, 28782, 994, 14343, 2038, 374, 2561, 382, 68744, 811, 2011, 1005, 279, 2539, 4113, 2316, 31503, 505, 279, 4729, 11, 369, 3187, 25, 47706, 330, 33, 598, 50153, 25, 578, 10657, 76508, 354, 507, 68070, 5169, 49105, 320, 791, 76508, 354, 507, 68070, 5169, 30727, 4804, 6017, 220, 17, 11844, 539, 264, 4096, 477, 66663, 2373, 315, 279, 2316, 382, 7, 19, 8, 6075, 58, 2831, 5787, 9356, 1217, 44983, 11, 3790, 1022, 86800, 24208, 11, 477, 6013, 311, 46305, 7540, 311, 10519, 264, 5933, 323, 56887, 10652, 6530, 382, 2675, 1253, 1935, 439, 1690, 7504, 439, 5995, 311, 8641, 279, 1217, 9017, 279, 1455, 14791, 28782, 13, 5321, 1988, 279, 1957, 6089, 11, 656, 539, 1988, 279, 5217, 330, 2573, 308, 3343, 70589, 1473, 38863, 367, 220, 15, 25, 28653, 1070, 0, 358, 2846, 304, 279, 20247, 369, 2555, 430, 63526, 264, 2697, 23347, 449, 1063, 13560, 26085, 0, 25545, 904, 19075, 5380, 2573, 220, 15, 25, 128009, 128006, 78191, 128007, 271, 27264, 58, 3923, 955, 315, 13560, 6376, 656, 499, 10932, 30, 1789, 3187, 11, 656, 499, 1093, 12688, 5895, 8329, 11, 43535, 10280, 7493, 11, 477, 8530, 264, 59937, 13970, 39577, 94068, 128009]
[INFO|configuration_utils.py:677] 2024-12-14 11:00:03,253 >> loading configuration file /data/pretrain_dir/Llama-3.1/config.json
[INFO|configuration_utils.py:746] 2024-12-14 11:00:03,255 >> Model config LlamaConfig {
  "_name_or_path": "/data/pretrain_dir/Llama-3.1",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3934] 2024-12-14 11:00:03,295 >> loading weights file /data/pretrain_dir/Llama-3.1/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2024-12-14 11:00:03,295 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2024-12-14 11:00:03,297 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ]
}

rejected_inputs:
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>

Solve recommendation tasks by interleaving "Observation" and "Action" steps.

Action: Choose one of the following actions based on the situation:
(1) Search[Keywords]: Search for relevant information using targeted keywords to aid in generating specific suggestions. 

(2) Ask[Question]: Interact with the user to clarify their preferences or constraints. Use strategic questioning to gradually uncover their requirements. 

(3) Recommend[Answer]: Provide recommendations based on available information and reasoning. Recommendations can serve two purposes:

Exploratory Purpose: Provide a tentative recommendation when information is incomplete to gather user feedback and refine preferences.
Final Purpose: Provide a clear and comprehensive recommendation when sufficient information is available.

Recommendations must use the full original title retrieved from the database, for example: Recommend "Banshee: The Second Dermot O'Hara Mystery (The Dermot O'Hara Mysteries Book 2)", not a description or shortened version of the title.

(4) Response[Content]: Address user inquiries, handle off-topic remarks, or respond to unrelated requests to maintain a natural and coherent conversation flow.

You may take as many steps as necessary to guide the user toward the most suitable recommendation. Please input the action directly, do not input the additional "Action n". Dialogue:

Observation 0: Hey there! I'm in the mood for something that mixes a little mystery with some holiday cheer! Got any recommendations?
Action 0:<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Ask[What type of holiday setting do you prefer? For example, do you like winter wonderlands, cozy Christmas stories, or perhaps a festive historical backdrop?]<|eot_id|>
rejected_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 27264, 58, 3923, 955, 315, 13560, 6376, 656, 499, 10932, 30, 1789, 3187, 11, 656, 499, 1093, 12688, 5895, 8329, 11, 43535, 10280, 7493, 11, 477, 8530, 264, 59937, 13970, 39577, 94068, 128009]
rejected_labels:
Ask[What type of holiday setting do you prefer? For example, do you like winter wonderlands, cozy Christmas stories, or perhaps a festive historical backdrop?]<|eot_id|>
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
[INFO|modeling_utils.py:4800] 2024-12-14 11:00:08,437 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4808] 2024-12-14 11:00:08,437 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /data/pretrain_dir/Llama-3.1.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1049] 2024-12-14 11:00:08,440 >> loading configuration file /data/pretrain_dir/Llama-3.1/generation_config.json
[INFO|configuration_utils.py:1096] 2024-12-14 11:00:08,440 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "temperature": 0.6,
  "top_p": 0.9
}

[INFO|2024-12-14 11:00:08] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2024-12-14 11:00:08] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2024-12-14 11:00:08] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2024-12-14 11:00:08] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2024-12-14 11:00:08] llamafactory.model.adapter:157 >> Loaded adapter(s): /data/fxy/ecpo/fine-tuning/sft/Book/lr5.0e-5_epochs3.0
[INFO|2024-12-14 11:00:08] llamafactory.model.model_utils.valuehead:157 >> Provided path (/data/fxy/ecpo/fine-tuning/sft/Book/lr5.0e-5_epochs3.0) does not contain value head weights: /data/fxy/ecpo/fine-tuning/sft/Book/lr5.0e-5_epochs3.0 does not appear to have a file named value_head.bin. Checkout 'https://huggingface.co//data/fxy/ecpo/fine-tuning/sft/Book/lr5.0e-5_epochs3.0/tree/None' for available files..
[INFO|2024-12-14 11:00:08] llamafactory.model.model_utils.valuehead:157 >> Ignore the above message if you are not resuming the training of a value head model.
[INFO|2024-12-14 11:00:09] llamafactory.model.loader:157 >> trainable params: 20,975,617 || all params: 8,051,236,865 || trainable%: 0.2605
/data/fxy/ecpo/LLaMA-Factory/src/llamafactory/train/rm/trainer.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `PairwiseTrainer.__init__`. Use `processing_class` instead.
  super().__init__(**kwargs)
[INFO|trainer.py:698] 2024-12-14 11:00:09,020 >> Using auto half precision backend
[INFO|trainer.py:2313] 2024-12-14 11:00:09,298 >> ***** Running training *****
[INFO|trainer.py:2314] 2024-12-14 11:00:09,298 >>   Num examples = 1,755
[INFO|trainer.py:2315] 2024-12-14 11:00:09,298 >>   Num Epochs = 3
[INFO|trainer.py:2316] 2024-12-14 11:00:09,298 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2319] 2024-12-14 11:00:09,298 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:2320] 2024-12-14 11:00:09,298 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2024-12-14 11:00:09,298 >>   Total optimization steps = 657
[INFO|trainer.py:2322] 2024-12-14 11:00:09,302 >>   Number of trainable parameters = 20,975,617
  0%|          | 0/657 [00:00<?, ?it/s]/data/fxy/anaconda3/envs/ecpo/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  0%|          | 1/657 [00:09<1:46:10,  9.71s/it]  0%|          | 2/657 [00:16<1:25:49,  7.86s/it]  0%|          | 3/657 [00:22<1:16:11,  6.99s/it]  1%|          | 4/657 [00:28<1:13:31,  6.76s/it]  1%|          | 5/657 [00:35<1:11:56,  6.62s/it]  1%|          | 6/657 [00:43<1:18:08,  7.20s/it]  1%|          | 7/657 [00:47<1:08:16,  6.30s/it]  1%|          | 8/657 [00:53<1:07:16,  6.22s/it]  1%|▏         | 9/657 [01:00<1:08:25,  6.34s/it]  2%|▏         | 10/657 [01:05<1:05:00,  6.03s/it]                                                    2%|▏         | 10/657 [01:05<1:05:00,  6.03s/it]  2%|▏         | 11/657 [01:13<1:08:58,  6.41s/it]  2%|▏         | 12/657 [01:21<1:14:07,  6.90s/it]  2%|▏         | 13/657 [01:29<1:18:43,  7.33s/it]  2%|▏         | 14/657 [01:37<1:19:57,  7.46s/it]  2%|▏         | 15/657 [01:43<1:17:22,  7.23s/it]  2%|▏         | 16/657 [01:48<1:10:11,  6.57s/it]  3%|▎         | 17/657 [01:55<1:11:05,  6.66s/it]  3%|▎         | 18/657 [02:03<1:14:40,  7.01s/it]  3%|▎         | 19/657 [02:13<1:23:05,  7.81s/it]  3%|▎         | 20/657 [02:20<1:20:52,  7.62s/it]                                                    3%|▎         | 20/657 [02:20<1:20:52,  7.62s/it]  3%|▎         | 21/657 [02:25<1:12:46,  6.87s/it]  3%|▎         | 22/657 [02:32<1:14:30,  7.04s/it]  4%|▎         | 23/657 [02:37<1:07:09,  6.36s/it]  4%|▎         | 24/657 [02:41<58:02,  5.50s/it]    4%|▍         | 25/657 [02:48<1:02:45,  5.96s/it]  4%|▍         | 26/657 [02:53<1:01:45,  5.87s/it]  4%|▍         | 27/657 [03:00<1:03:46,  6.07s/it]  4%|▍         | 28/657 [03:05<1:01:41,  5.88s/it]  4%|▍         | 29/657 [03:11<1:00:27,  5.78s/it]  5%|▍         | 30/657 [03:19<1:05:58,  6.31s/it]                                                    5%|▍         | 30/657 [03:19<1:05:58,  6.31s/it]  5%|▍         | 31/657 [03:25<1:05:47,  6.31s/it]  5%|▍         | 32/657 [03:32<1:08:37,  6.59s/it]  5%|▌         | 33/657 [03:39<1:10:36,  6.79s/it]  5%|▌         | 34/657 [03:46<1:10:03,  6.75s/it]  5%|▌         | 35/657 [03:51<1:05:45,  6.34s/it]  5%|▌         | 36/657 [03:58<1:06:00,  6.38s/it]  6%|▌         | 37/657 [04:06<1:10:00,  6.78s/it]  6%|▌         | 38/657 [04:13<1:11:03,  6.89s/it]  6%|▌         | 39/657 [04:19<1:09:11,  6.72s/it]  6%|▌         | 40/657 [04:25<1:05:22,  6.36s/it]                                                    6%|▌         | 40/657 [04:25<1:05:22,  6.36s/it]  6%|▌         | 41/657 [04:31<1:06:07,  6.44s/it]  6%|▋         | 42/657 [04:39<1:09:13,  6.75s/it]  7%|▋         | 43/657 [04:45<1:06:36,  6.51s/it]  7%|▋         | 44/657 [04:51<1:05:15,  6.39s/it]  7%|▋         | 45/657 [04:58<1:07:18,  6.60s/it]  7%|▋         | 46/657 [05:04<1:04:54,  6.37s/it]  7%|▋         | 47/657 [05:10<1:05:35,  6.45s/it]  7%|▋         | 48/657 [05:16<1:04:44,  6.38s/it]  7%|▋         | 49/657 [05:24<1:07:25,  6.65s/it]  8%|▊         | 50/657 [05:31<1:07:47,  6.70s/it]                                                    8%|▊         | 50/657 [05:31<1:07:47,  6.70s/it]  8%|▊         | 51/657 [05:39<1:12:25,  7.17s/it]  8%|▊         | 52/657 [05:45<1:08:28,  6.79s/it]  8%|▊         | 53/657 [05:50<1:04:00,  6.36s/it]  8%|▊         | 54/657 [05:57<1:04:47,  6.45s/it]  8%|▊         | 55/657 [06:03<1:04:27,  6.42s/it]  9%|▊         | 56/657 [06:12<1:12:05,  7.20s/it]  9%|▊         | 57/657 [06:19<1:09:55,  6.99s/it]  9%|▉         | 58/657 [06:29<1:19:42,  7.98s/it]  9%|▉         | 59/657 [06:36<1:16:52,  7.71s/it]  9%|▉         | 60/657 [06:43<1:16:01,  7.64s/it]                                                    9%|▉         | 60/657 [06:43<1:16:01,  7.64s/it]  9%|▉         | 61/657 [06:49<1:08:54,  6.94s/it]  9%|▉         | 62/657 [06:55<1:05:41,  6.63s/it] 10%|▉         | 63/657 [07:04<1:12:52,  7.36s/it] 10%|▉         | 64/657 [07:13<1:18:17,  7.92s/it] 10%|▉         | 65/657 [07:20<1:15:31,  7.65s/it] 10%|█         | 66/657 [07:25<1:07:50,  6.89s/it] 10%|█         | 67/657 [07:31<1:05:34,  6.67s/it] 10%|█         | 68/657 [07:39<1:09:03,  7.03s/it] 11%|█         | 69/657 [07:48<1:12:55,  7.44s/it] 11%|█         | 70/657 [07:53<1:06:49,  6.83s/it]                                                   11%|█         | 70/657 [07:53<1:06:49,  6.83s/it] 11%|█         | 71/657 [07:59<1:03:15,  6.48s/it] 11%|█         | 72/657 [08:05<1:03:15,  6.49s/it] 11%|█         | 73/657 [08:12<1:03:21,  6.51s/it] 11%|█▏        | 74/657 [08:20<1:08:23,  7.04s/it] 11%|█▏        | 75/657 [08:27<1:07:34,  6.97s/it] 12%|█▏        | 76/657 [08:34<1:08:03,  7.03s/it] 12%|█▏        | 77/657 [08:40<1:05:58,  6.83s/it] 12%|█▏        | 78/657 [08:50<1:13:11,  7.59s/it] 12%|█▏        | 79/657 [08:55<1:05:23,  6.79s/it] 12%|█▏        | 80/657 [09:01<1:04:33,  6.71s/it]                                                   12%|█▏        | 80/657 [09:01<1:04:33,  6.71s/it] 12%|█▏        | 81/657 [09:09<1:06:43,  6.95s/it] 12%|█▏        | 82/657 [09:15<1:05:07,  6.80s/it] 13%|█▎        | 83/657 [09:20<59:10,  6.19s/it]   13%|█▎        | 84/657 [09:26<59:04,  6.19s/it] 13%|█▎        | 85/657 [09:33<1:02:05,  6.51s/it] 13%|█▎        | 86/657 [09:41<1:05:36,  6.89s/it] 13%|█▎        | 87/657 [09:49<1:07:22,  7.09s/it] 13%|█▎        | 88/657 [09:53<1:00:48,  6.41s/it] 14%|█▎        | 89/657 [10:02<1:05:41,  6.94s/it] 14%|█▎        | 90/657 [10:07<1:01:54,  6.55s/it]                                                   14%|█▎        | 90/657 [10:07<1:01:54,  6.55s/it] 14%|█▍        | 91/657 [10:12<57:52,  6.13s/it]   14%|█▍        | 92/657 [10:19<57:48,  6.14s/it] 14%|█▍        | 93/657 [10:24<54:58,  5.85s/it] 14%|█▍        | 94/657 [10:30<56:26,  6.02s/it] 14%|█▍        | 95/657 [10:38<1:01:42,  6.59s/it] 15%|█▍        | 96/657 [10:44<1:00:36,  6.48s/it] 15%|█▍        | 97/657 [10:52<1:02:58,  6.75s/it] 15%|█▍        | 98/657 [10:59<1:04:08,  6.88s/it] 15%|█▌        | 99/657 [11:08<1:11:18,  7.67s/it] 15%|█▌        | 100/657 [11:16<1:12:20,  7.79s/it]                                                    15%|█▌        | 100/657 [11:16<1:12:20,  7.79s/it] 15%|█▌        | 101/657 [11:21<1:02:28,  6.74s/it] 16%|█▌        | 102/657 [11:29<1:05:28,  7.08s/it] 16%|█▌        | 103/657 [11:34<1:00:51,  6.59s/it] 16%|█▌        | 104/657 [11:40<58:01,  6.30s/it]   16%|█▌        | 105/657 [11:46<57:25,  6.24s/it] 16%|█▌        | 106/657 [11:54<1:02:47,  6.84s/it] 16%|█▋        | 107/657 [12:01<1:04:13,  7.01s/it] 16%|█▋        | 108/657 [12:08<1:04:05,  7.00s/it] 17%|█▋        | 109/657 [12:12<53:24,  5.85s/it]   17%|█▋        | 110/657 [12:19<57:10,  6.27s/it]                                                  17%|█▋        | 110/657 [12:19<57:10,  6.27s/it] 17%|█▋        | 111/657 [12:27<1:01:52,  6.80s/it] 17%|█▋        | 112/657 [12:35<1:04:34,  7.11s/it] 17%|█▋        | 113/657 [12:40<58:30,  6.45s/it]   17%|█▋        | 114/657 [12:46<57:36,  6.37s/it] 18%|█▊        | 115/657 [12:56<1:07:48,  7.51s/it] 18%|█▊        | 116/657 [13:04<1:07:55,  7.53s/it] 18%|█▊        | 117/657 [13:09<1:02:50,  6.98s/it] 18%|█▊        | 118/657 [13:15<1:00:37,  6.75s/it] 18%|█▊        | 119/657 [13:20<55:49,  6.23s/it]   18%|█▊        | 120/657 [13:26<55:20,  6.18s/it]                                                  18%|█▊        | 120/657 [13:26<55:20,  6.18s/it] 18%|█▊        | 121/657 [13:33<57:17,  6.41s/it] 19%|█▊        | 122/657 [13:37<49:56,  5.60s/it] 19%|█▊        | 123/657 [13:41<45:45,  5.14s/it] 19%|█▉        | 124/657 [13:50<56:40,  6.38s/it] 19%|█▉        | 125/657 [13:55<51:34,  5.82s/it] 19%|█▉        | 126/657 [14:02<53:58,  6.10s/it] 19%|█▉        | 127/657 [14:07<52:00,  5.89s/it] 19%|█▉        | 128/657 [14:16<58:44,  6.66s/it] 20%|█▉        | 129/657 [14:24<1:02:31,  7.10s/it] 20%|█▉        | 130/657 [14:30<1:01:12,  6.97s/it]                                                    20%|█▉        | 130/657 [14:30<1:01:12,  6.97s/it] 20%|█▉        | 131/657 [14:40<1:08:59,  7.87s/it] 20%|██        | 132/657 [14:43<55:29,  6.34s/it]   20%|██        | 133/657 [14:52<1:02:26,  7.15s/it] 20%|██        | 134/657 [15:00<1:02:49,  7.21s/it] 21%|██        | 135/657 [15:06<1:00:07,  6.91s/it] 21%|██        | 136/657 [15:14<1:04:42,  7.45s/it] 21%|██        | 137/657 [15:20<59:28,  6.86s/it]   21%|██        | 138/657 [15:28<1:02:12,  7.19s/it] 21%|██        | 139/657 [15:32<54:52,  6.36s/it]   21%|██▏       | 140/657 [15:41<1:01:38,  7.15s/it]                                                    21%|██▏       | 140/657 [15:41<1:01:38,  7.15s/it] 21%|██▏       | 141/657 [15:47<57:56,  6.74s/it]   22%|██▏       | 142/657 [15:55<1:01:03,  7.11s/it] 22%|██▏       | 143/657 [16:03<1:01:46,  7.21s/it] 22%|██▏       | 144/657 [16:09<59:47,  6.99s/it]   22%|██▏       | 145/657 [16:14<54:49,  6.43s/it] 22%|██▏       | 146/657 [16:19<51:08,  6.00s/it] 22%|██▏       | 147/657 [16:28<57:49,  6.80s/it] 23%|██▎       | 148/657 [16:35<58:42,  6.92s/it] 23%|██▎       | 149/657 [16:43<1:00:07,  7.10s/it] 23%|██▎       | 150/657 [16:47<54:28,  6.45s/it]                                                    23%|██▎       | 150/657 [16:47<54:28,  6.45s/it] 23%|██▎       | 151/657 [16:54<53:58,  6.40s/it] 23%|██▎       | 152/657 [17:00<52:59,  6.30s/it] 23%|██▎       | 153/657 [17:06<53:38,  6.39s/it] 23%|██▎       | 154/657 [17:15<58:14,  6.95s/it] 24%|██▎       | 155/657 [17:20<54:02,  6.46s/it] 24%|██▎       | 156/657 [17:28<57:59,  6.95s/it] 24%|██▍       | 157/657 [17:35<56:54,  6.83s/it] 24%|██▍       | 158/657 [17:41<55:28,  6.67s/it] 24%|██▍       | 159/657 [17:47<54:21,  6.55s/it] 24%|██▍       | 160/657 [17:53<53:28,  6.45s/it]                                                  24%|██▍       | 160/657 [17:53<53:28,  6.45s/it] 25%|██▍       | 161/657 [18:00<52:34,  6.36s/it] 25%|██▍       | 162/657 [18:07<54:55,  6.66s/it] 25%|██▍       | 163/657 [18:15<57:12,  6.95s/it] 25%|██▍       | 164/657 [18:21<56:20,  6.86s/it] 25%|██▌       | 165/657 [18:28<57:17,  6.99s/it] 25%|██▌       | 166/657 [18:36<59:19,  7.25s/it] 25%|██▌       | 167/657 [18:43<58:04,  7.11s/it] 26%|██▌       | 168/657 [18:51<1:00:26,  7.42s/it] 26%|██▌       | 169/657 [19:01<1:05:46,  8.09s/it] 26%|██▌       | 170/657 [19:07<59:58,  7.39s/it]                                                    26%|██▌       | 170/657 [19:07<59:58,  7.39s/it] 26%|██▌       | 171/657 [19:13<56:48,  7.01s/it] 26%|██▌       | 172/657 [19:20<57:42,  7.14s/it] 26%|██▋       | 173/657 [19:27<55:41,  6.90s/it] 26%|██▋       | 174/657 [19:33<54:59,  6.83s/it] 27%|██▋       | 175/657 [19:41<56:08,  6.99s/it] 27%|██▋       | 176/657 [19:45<49:30,  6.18s/it] 27%|██▋       | 177/657 [19:53<54:07,  6.77s/it] 27%|██▋       | 178/657 [20:00<53:42,  6.73s/it] 27%|██▋       | 179/657 [20:07<54:57,  6.90s/it] 27%|██▋       | 180/657 [20:13<53:09,  6.69s/it]                                                  27%|██▋       | 180/657 [20:13<53:09,  6.69s/it] 28%|██▊       | 181/657 [20:22<57:31,  7.25s/it] 28%|██▊       | 182/657 [20:28<55:08,  6.97s/it] 28%|██▊       | 183/657 [20:35<55:57,  7.08s/it] 28%|██▊       | 184/657 [20:41<52:07,  6.61s/it] 28%|██▊       | 185/657 [20:49<55:38,  7.07s/it] 28%|██▊       | 186/657 [20:54<51:31,  6.56s/it] 28%|██▊       | 187/657 [21:01<52:12,  6.66s/it] 29%|██▊       | 188/657 [21:10<57:14,  7.32s/it] 29%|██▉       | 189/657 [21:17<56:24,  7.23s/it] 29%|██▉       | 190/657 [21:24<54:50,  7.05s/it]                                                  29%|██▉       | 190/657 [21:24<54:50,  7.05s/it] 29%|██▉       | 191/657 [21:30<51:49,  6.67s/it] 29%|██▉       | 192/657 [21:35<48:28,  6.25s/it] 29%|██▉       | 193/657 [21:41<47:40,  6.17s/it] 30%|██▉       | 194/657 [21:46<45:23,  5.88s/it] 30%|██▉       | 195/657 [21:52<46:09,  5.99s/it] 30%|██▉       | 196/657 [22:00<49:59,  6.51s/it] 30%|██▉       | 197/657 [22:04<42:59,  5.61s/it] 30%|███       | 198/657 [22:09<43:23,  5.67s/it] 30%|███       | 199/657 [22:15<43:19,  5.68s/it] 30%|███       | 200/657 [22:21<44:23,  5.83s/it]                                                  30%|███       | 200/657 [22:21<44:23,  5.83s/it] 31%|███       | 201/657 [22:28<45:53,  6.04s/it] 31%|███       | 202/657 [22:34<46:08,  6.08s/it] 31%|███       | 203/657 [22:40<46:19,  6.12s/it] 31%|███       | 204/657 [22:45<44:24,  5.88s/it] 31%|███       | 205/657 [22:51<43:15,  5.74s/it] 31%|███▏      | 206/657 [22:55<39:21,  5.24s/it] 32%|███▏      | 207/657 [23:00<37:55,  5.06s/it] 32%|███▏      | 208/657 [23:05<38:16,  5.11s/it] 32%|███▏      | 209/657 [23:13<45:39,  6.12s/it] 32%|███▏      | 210/657 [23:19<45:19,  6.08s/it]                                                  32%|███▏      | 210/657 [23:19<45:19,  6.08s/it] 32%|███▏      | 211/657 [23:25<44:45,  6.02s/it] 32%|███▏      | 212/657 [23:30<43:03,  5.81s/it] 32%|███▏      | 213/657 [23:38<46:03,  6.22s/it] 33%|███▎      | 214/657 [23:44<45:43,  6.19s/it] 33%|███▎      | 215/657 [23:51<48:23,  6.57s/it] 33%|███▎      | 216/657 [23:57<45:45,  6.23s/it] 33%|███▎      | 217/657 [24:01<41:28,  5.66s/it] 33%|███▎      | 218/657 [24:06<40:31,  5.54s/it] 33%|███▎      | 219/657 [24:16<48:49,  6.69s/it] 33%|███▎      | 220/657 [24:21<46:42,  6.41s/it]                                                  33%|███▎      | 220/657 [24:21<46:42,  6.41s/it] 34%|███▎      | 221/657 [24:26<43:20,  5.96s/it] 34%|███▍      | 222/657 [24:33<45:55,  6.33s/it] 34%|███▍      | 223/657 [24:43<51:49,  7.17s/it] 34%|███▍      | 224/657 [24:48<47:36,  6.60s/it] 34%|███▍      | 225/657 [24:56<51:27,  7.15s/it] 34%|███▍      | 226/657 [25:01<45:51,  6.38s/it] 35%|███▍      | 227/657 [25:09<48:49,  6.81s/it] 35%|███▍      | 228/657 [25:15<46:43,  6.54s/it] 35%|███▍      | 229/657 [25:23<51:07,  7.17s/it] 35%|███▌      | 230/657 [25:31<52:32,  7.38s/it]                                                  35%|███▌      | 230/657 [25:31<52:32,  7.38s/it] 35%|███▌      | 231/657 [25:37<49:01,  6.90s/it] 35%|███▌      | 232/657 [25:46<53:09,  7.50s/it] 35%|███▌      | 233/657 [25:52<49:17,  6.97s/it] 36%|███▌      | 234/657 [25:56<44:51,  6.36s/it] 36%|███▌      | 235/657 [26:02<43:02,  6.12s/it] 36%|███▌      | 236/657 [26:07<40:29,  5.77s/it] 36%|███▌      | 237/657 [26:13<41:32,  5.93s/it] 36%|███▌      | 238/657 [26:20<43:22,  6.21s/it] 36%|███▋      | 239/657 [26:28<46:13,  6.64s/it] 37%|███▋      | 240/657 [26:32<41:49,  6.02s/it]                                                  37%|███▋      | 240/657 [26:32<41:49,  6.02s/it] 37%|███▋      | 241/657 [26:38<41:45,  6.02s/it] 37%|███▋      | 242/657 [26:48<49:56,  7.22s/it] 37%|███▋      | 243/657 [26:53<43:39,  6.33s/it] 37%|███▋      | 244/657 [26:58<41:59,  6.10s/it] 37%|███▋      | 245/657 [27:04<41:29,  6.04s/it] 37%|███▋      | 246/657 [27:09<39:50,  5.82s/it] 38%|███▊      | 247/657 [27:17<43:50,  6.42s/it] 38%|███▊      | 248/657 [27:24<44:30,  6.53s/it] 38%|███▊      | 249/657 [27:30<43:35,  6.41s/it] 38%|███▊      | 250/657 [27:37<44:08,  6.51s/it]                                                  38%|███▊      | 250/657 [27:37<44:08,  6.51s/it] 38%|███▊      | 251/657 [27:46<49:16,  7.28s/it] 38%|███▊      | 252/657 [27:53<48:13,  7.14s/it] 39%|███▊      | 253/657 [28:02<51:22,  7.63s/it] 39%|███▊      | 254/657 [28:09<50:34,  7.53s/it] 39%|███▉      | 255/657 [28:18<52:46,  7.88s/it] 39%|███▉      | 256/657 [28:26<53:54,  8.07s/it] 39%|███▉      | 257/657 [28:34<53:51,  8.08s/it] 39%|███▉      | 258/657 [28:39<47:56,  7.21s/it] 39%|███▉      | 259/657 [28:47<48:27,  7.30s/it] 40%|███▉      | 260/657 [28:55<50:21,  7.61s/it]                                                  40%|███▉      | 260/657 [28:55<50:21,  7.61s/it] 40%|███▉      | 261/657 [29:03<50:10,  7.60s/it] 40%|███▉      | 262/657 [29:08<45:58,  6.98s/it] 40%|████      | 263/657 [29:18<50:52,  7.75s/it] 40%|████      | 264/657 [29:26<51:17,  7.83s/it] 40%|████      | 265/657 [29:30<43:50,  6.71s/it] 40%|████      | 266/657 [29:36<41:57,  6.44s/it] 41%|████      | 267/657 [29:42<41:29,  6.38s/it] 41%|████      | 268/657 [29:49<43:06,  6.65s/it] 41%|████      | 269/657 [29:57<45:38,  7.06s/it] 41%|████      | 270/657 [30:03<42:00,  6.51s/it]                                                  41%|████      | 270/657 [30:03<42:00,  6.51s/it] 41%|████      | 271/657 [30:10<42:47,  6.65s/it] 41%|████▏     | 272/657 [30:15<39:59,  6.23s/it] 42%|████▏     | 273/657 [30:21<40:13,  6.28s/it] 42%|████▏     | 274/657 [30:26<36:59,  5.80s/it] 42%|████▏     | 275/657 [30:33<39:40,  6.23s/it] 42%|████▏     | 276/657 [30:43<47:00,  7.40s/it] 42%|████▏     | 277/657 [30:50<45:37,  7.20s/it] 42%|████▏     | 278/657 [30:57<45:22,  7.18s/it] 42%|████▏     | 279/657 [31:01<38:05,  6.05s/it] 43%|████▎     | 280/657 [31:10<44:20,  7.06s/it]                                                  43%|████▎     | 280/657 [31:10<44:20,  7.06s/it] 43%|████▎     | 281/657 [31:18<45:20,  7.24s/it] 43%|████▎     | 282/657 [31:24<43:25,  6.95s/it] 43%|████▎     | 283/657 [31:30<42:05,  6.75s/it] 43%|████▎     | 284/657 [31:37<42:10,  6.78s/it] 43%|████▎     | 285/657 [31:43<40:04,  6.46s/it] 44%|████▎     | 286/657 [31:50<42:19,  6.84s/it] 44%|████▎     | 287/657 [31:55<38:11,  6.19s/it] 44%|████▍     | 288/657 [32:03<41:34,  6.76s/it] 44%|████▍     | 289/657 [32:10<41:46,  6.81s/it] 44%|████▍     | 290/657 [32:16<39:12,  6.41s/it]                                                  44%|████▍     | 290/657 [32:16<39:12,  6.41s/it] 44%|████▍     | 291/657 [32:22<39:10,  6.42s/it] 44%|████▍     | 292/657 [32:31<43:43,  7.19s/it] 45%|████▍     | 293/657 [32:37<41:40,  6.87s/it] 45%|████▍     | 294/657 [32:46<45:13,  7.48s/it] 45%|████▍     | 295/657 [32:53<43:42,  7.25s/it] 45%|████▌     | 296/657 [32:59<40:59,  6.81s/it] 45%|████▌     | 297/657 [33:04<38:04,  6.35s/it] 45%|████▌     | 298/657 [33:10<37:27,  6.26s/it] 46%|████▌     | 299/657 [33:18<40:44,  6.83s/it] 46%|████▌     | 300/657 [33:26<41:56,  7.05s/it]                                                  46%|████▌     | 300/657 [33:26<41:56,  7.05s/it] 46%|████▌     | 301/657 [33:31<38:02,  6.41s/it] 46%|████▌     | 302/657 [33:37<37:35,  6.35s/it] 46%|████▌     | 303/657 [33:40<31:55,  5.41s/it] 46%|████▋     | 304/657 [33:45<31:08,  5.29s/it] 46%|████▋     | 305/657 [33:51<32:03,  5.46s/it] 47%|████▋     | 306/657 [33:59<35:50,  6.13s/it] 47%|████▋     | 307/657 [34:05<36:21,  6.23s/it] 47%|████▋     | 308/657 [34:11<36:14,  6.23s/it] 47%|████▋     | 309/657 [34:19<38:13,  6.59s/it] 47%|████▋     | 310/657 [34:25<37:39,  6.51s/it]                                                  47%|████▋     | 310/657 [34:25<37:39,  6.51s/it] 47%|████▋     | 311/657 [34:29<33:48,  5.86s/it] 47%|████▋     | 312/657 [34:37<36:57,  6.43s/it] 48%|████▊     | 313/657 [34:41<32:51,  5.73s/it] 48%|████▊     | 314/657 [34:49<36:15,  6.34s/it] 48%|████▊     | 315/657 [34:55<35:43,  6.27s/it] 48%|████▊     | 316/657 [35:01<35:13,  6.20s/it] 48%|████▊     | 317/657 [35:07<34:55,  6.16s/it] 48%|████▊     | 318/657 [35:12<31:59,  5.66s/it] 49%|████▊     | 319/657 [35:19<35:25,  6.29s/it] 49%|████▊     | 320/657 [35:26<35:10,  6.26s/it]                                                  49%|████▊     | 320/657 [35:26<35:10,  6.26s/it] 49%|████▉     | 321/657 [35:34<38:01,  6.79s/it] 49%|████▉     | 322/657 [35:40<36:54,  6.61s/it] 49%|████▉     | 323/657 [35:45<35:15,  6.33s/it] 49%|████▉     | 324/657 [35:53<37:54,  6.83s/it] 49%|████▉     | 325/657 [35:58<34:16,  6.20s/it] 50%|████▉     | 326/657 [36:05<35:08,  6.37s/it] 50%|████▉     | 327/657 [36:12<36:24,  6.62s/it] 50%|████▉     | 328/657 [36:19<37:02,  6.76s/it] 50%|█████     | 329/657 [36:27<38:42,  7.08s/it] 50%|█████     | 330/657 [36:34<38:02,  6.98s/it]                                                  50%|█████     | 330/657 [36:34<38:02,  6.98s/it] 50%|█████     | 331/657 [36:41<38:55,  7.17s/it] 51%|█████     | 332/657 [36:48<37:38,  6.95s/it] 51%|█████     | 333/657 [36:52<33:04,  6.13s/it] 51%|█████     | 334/657 [36:59<33:45,  6.27s/it] 51%|█████     | 335/657 [37:05<33:28,  6.24s/it] 51%|█████     | 336/657 [37:12<35:36,  6.66s/it] 51%|█████▏    | 337/657 [37:20<37:39,  7.06s/it] 51%|█████▏    | 338/657 [37:29<40:28,  7.61s/it] 52%|█████▏    | 339/657 [37:38<42:02,  7.93s/it] 52%|█████▏    | 340/657 [37:42<34:52,  6.60s/it]                                                  52%|█████▏    | 340/657 [37:42<34:52,  6.60s/it] 52%|█████▏    | 341/657 [37:49<36:08,  6.86s/it] 52%|█████▏    | 342/657 [37:54<32:31,  6.19s/it] 52%|█████▏    | 343/657 [38:01<33:41,  6.44s/it] 52%|█████▏    | 344/657 [38:09<36:39,  7.03s/it] 53%|█████▎    | 345/657 [38:14<33:56,  6.53s/it] 53%|█████▎    | 346/657 [38:21<33:47,  6.52s/it] 53%|█████▎    | 347/657 [38:28<34:54,  6.76s/it] 53%|█████▎    | 348/657 [38:36<36:07,  7.01s/it] 53%|█████▎    | 349/657 [38:39<30:44,  5.99s/it] 53%|█████▎    | 350/657 [38:46<31:32,  6.16s/it]                                                  53%|█████▎    | 350/657 [38:46<31:32,  6.16s/it] 53%|█████▎    | 351/657 [38:56<36:57,  7.25s/it] 54%|█████▎    | 352/657 [39:00<31:56,  6.28s/it] 54%|█████▎    | 353/657 [39:06<31:49,  6.28s/it] 54%|█████▍    | 354/657 [39:14<34:02,  6.74s/it] 54%|█████▍    | 355/657 [39:21<35:09,  6.98s/it] 54%|█████▍    | 356/657 [39:28<33:40,  6.71s/it] 54%|█████▍    | 357/657 [39:34<33:16,  6.66s/it] 54%|█████▍    | 358/657 [39:43<35:49,  7.19s/it] 55%|█████▍    | 359/657 [39:50<36:22,  7.32s/it] 55%|█████▍    | 360/657 [39:56<34:02,  6.88s/it]                                                  55%|█████▍    | 360/657 [39:56<34:02,  6.88s/it] 55%|█████▍    | 361/657 [40:01<31:35,  6.40s/it] 55%|█████▌    | 362/657 [40:09<34:00,  6.92s/it] 55%|█████▌    | 363/657 [40:16<33:50,  6.91s/it] 55%|█████▌    | 364/657 [40:25<36:26,  7.46s/it] 56%|█████▌    | 365/657 [40:31<34:28,  7.08s/it] 56%|█████▌    | 366/657 [40:36<30:30,  6.29s/it] 56%|█████▌    | 367/657 [40:42<30:53,  6.39s/it] 56%|█████▌    | 368/657 [40:49<31:09,  6.47s/it] 56%|█████▌    | 369/657 [40:54<29:08,  6.07s/it] 56%|█████▋    | 370/657 [40:59<28:03,  5.87s/it]                                                  56%|█████▋    | 370/657 [40:59<28:03,  5.87s/it] 56%|█████▋    | 371/657 [41:07<30:28,  6.39s/it] 57%|█████▋    | 372/657 [41:16<33:32,  7.06s/it] 57%|█████▋    | 373/657 [41:22<32:44,  6.92s/it] 57%|█████▋    | 374/657 [41:29<32:37,  6.92s/it] 57%|█████▋    | 375/657 [41:35<30:59,  6.59s/it] 57%|█████▋    | 376/657 [41:42<30:46,  6.57s/it] 57%|█████▋    | 377/657 [41:48<30:16,  6.49s/it] 58%|█████▊    | 378/657 [41:55<31:18,  6.73s/it] 58%|█████▊    | 379/657 [42:01<30:08,  6.50s/it] 58%|█████▊    | 380/657 [42:08<31:01,  6.72s/it]                                                  58%|█████▊    | 380/657 [42:08<31:01,  6.72s/it] 58%|█████▊    | 381/657 [42:15<31:20,  6.81s/it] 58%|█████▊    | 382/657 [42:22<30:35,  6.67s/it] 58%|█████▊    | 383/657 [42:30<32:47,  7.18s/it] 58%|█████▊    | 384/657 [42:37<32:11,  7.07s/it] 59%|█████▊    | 385/657 [42:41<28:13,  6.22s/it] 59%|█████▉    | 386/657 [42:49<30:30,  6.75s/it] 59%|█████▉    | 387/657 [42:55<28:56,  6.43s/it] 59%|█████▉    | 388/657 [43:02<29:54,  6.67s/it] 59%|█████▉    | 389/657 [43:07<27:33,  6.17s/it] 59%|█████▉    | 390/657 [43:13<26:52,  6.04s/it]                                                  59%|█████▉    | 390/657 [43:13<26:52,  6.04s/it] 60%|█████▉    | 391/657 [43:20<28:07,  6.34s/it] 60%|█████▉    | 392/657 [43:27<28:30,  6.46s/it] 60%|█████▉    | 393/657 [43:31<25:34,  5.81s/it] 60%|█████▉    | 394/657 [43:37<25:52,  5.90s/it] 60%|██████    | 395/657 [43:44<27:07,  6.21s/it] 60%|██████    | 396/657 [43:52<28:46,  6.62s/it] 60%|██████    | 397/657 [43:57<27:09,  6.27s/it] 61%|██████    | 398/657 [44:01<24:42,  5.72s/it] 61%|██████    | 399/657 [44:10<28:12,  6.56s/it] 61%|██████    | 400/657 [44:17<28:18,  6.61s/it]                                                  61%|██████    | 400/657 [44:17<28:18,  6.61s/it] 61%|██████    | 401/657 [44:24<29:42,  6.96s/it] 61%|██████    | 402/657 [44:31<29:25,  6.92s/it] 61%|██████▏   | 403/657 [44:37<27:19,  6.45s/it] 61%|██████▏   | 404/657 [44:43<27:19,  6.48s/it] 62%|██████▏   | 405/657 [44:52<30:35,  7.28s/it] 62%|██████▏   | 406/657 [45:00<31:21,  7.50s/it] 62%|██████▏   | 407/657 [45:07<30:20,  7.28s/it] 62%|██████▏   | 408/657 [45:12<27:26,  6.61s/it] 62%|██████▏   | 409/657 [45:20<29:24,  7.12s/it] 62%|██████▏   | 410/657 [45:29<30:31,  7.41s/it]                                                  62%|██████▏   | 410/657 [45:29<30:31,  7.41s/it] 63%|██████▎   | 411/657 [45:36<31:02,  7.57s/it] 63%|██████▎   | 412/657 [45:45<32:04,  7.85s/it] 63%|██████▎   | 413/657 [45:53<32:10,  7.91s/it] 63%|██████▎   | 414/657 [46:00<30:48,  7.61s/it] 63%|██████▎   | 415/657 [46:06<28:56,  7.17s/it] 63%|██████▎   | 416/657 [46:14<29:44,  7.41s/it] 63%|██████▎   | 417/657 [46:19<26:30,  6.63s/it] 64%|██████▎   | 418/657 [46:26<27:11,  6.83s/it] 64%|██████▍   | 419/657 [46:31<24:42,  6.23s/it] 64%|██████▍   | 420/657 [46:38<25:56,  6.57s/it]                                                  64%|██████▍   | 420/657 [46:38<25:56,  6.57s/it] 64%|██████▍   | 421/657 [46:43<23:28,  5.97s/it] 64%|██████▍   | 422/657 [46:46<20:15,  5.17s/it] 64%|██████▍   | 423/657 [46:52<20:58,  5.38s/it] 65%|██████▍   | 424/657 [46:59<22:34,  5.82s/it] 65%|██████▍   | 425/657 [47:05<22:37,  5.85s/it] 65%|██████▍   | 426/657 [47:11<22:16,  5.79s/it] 65%|██████▍   | 427/657 [47:18<24:11,  6.31s/it] 65%|██████▌   | 428/657 [47:24<23:10,  6.07s/it] 65%|██████▌   | 429/657 [47:31<24:57,  6.57s/it] 65%|██████▌   | 430/657 [47:38<24:35,  6.50s/it]                                                  65%|██████▌   | 430/657 [47:38<24:35,  6.50s/it] 66%|██████▌   | 431/657 [47:45<25:26,  6.75s/it] 66%|██████▌   | 432/657 [47:51<24:44,  6.60s/it] 66%|██████▌   | 433/657 [47:59<25:46,  6.91s/it] 66%|██████▌   | 434/657 [48:05<24:55,  6.71s/it] 66%|██████▌   | 435/657 [48:12<25:21,  6.85s/it] 66%|██████▋   | 436/657 [48:19<24:48,  6.73s/it] 67%|██████▋   | 437/657 [48:25<24:04,  6.57s/it] 67%|██████▋   | 438/657 [48:31<23:52,  6.54s/it] 67%|██████▋   | 439/657 [48:37<22:28,  6.18s/it] 67%|██████▋   | 440/657 [48:45<24:54,  6.89s/it]                                                  67%|██████▋   | 440/657 [48:45<24:54,  6.89s/it] 67%|██████▋   | 441/657 [48:51<23:06,  6.42s/it] 67%|██████▋   | 442/657 [48:58<23:35,  6.58s/it] 67%|██████▋   | 443/657 [49:03<22:25,  6.29s/it] 68%|██████▊   | 444/657 [49:08<21:13,  5.98s/it] 68%|██████▊   | 445/657 [49:14<20:59,  5.94s/it] 68%|██████▊   | 446/657 [49:22<22:52,  6.50s/it] 68%|██████▊   | 447/657 [49:30<24:01,  6.86s/it] 68%|██████▊   | 448/657 [49:38<24:49,  7.12s/it] 68%|██████▊   | 449/657 [49:44<24:32,  7.08s/it] 68%|██████▊   | 450/657 [49:50<22:23,  6.49s/it]                                                  68%|██████▊   | 450/657 [49:50<22:23,  6.49s/it] 69%|██████▊   | 451/657 [49:56<22:11,  6.46s/it] 69%|██████▉   | 452/657 [50:03<22:13,  6.51s/it] 69%|██████▉   | 453/657 [50:09<21:41,  6.38s/it] 69%|██████▉   | 454/657 [50:14<20:40,  6.11s/it] 69%|██████▉   | 455/657 [50:22<22:34,  6.70s/it] 69%|██████▉   | 456/657 [50:29<22:00,  6.57s/it] 70%|██████▉   | 457/657 [50:34<20:19,  6.10s/it] 70%|██████▉   | 458/657 [50:41<22:03,  6.65s/it] 70%|██████▉   | 459/657 [50:50<23:42,  7.18s/it] 70%|███████   | 460/657 [50:56<22:32,  6.87s/it]                                                  70%|███████   | 460/657 [50:56<22:32,  6.87s/it] 70%|███████   | 461/657 [51:02<21:38,  6.62s/it] 70%|███████   | 462/657 [51:09<21:23,  6.58s/it] 70%|███████   | 463/657 [51:16<22:15,  6.88s/it] 71%|███████   | 464/657 [51:24<22:39,  7.04s/it] 71%|███████   | 465/657 [51:31<22:49,  7.13s/it] 71%|███████   | 466/657 [51:39<23:20,  7.33s/it] 71%|███████   | 467/657 [51:47<23:43,  7.49s/it] 71%|███████   | 468/657 [51:52<21:21,  6.78s/it] 71%|███████▏  | 469/657 [51:58<20:25,  6.52s/it] 72%|███████▏  | 470/657 [52:04<20:09,  6.47s/it]                                                  72%|███████▏  | 470/657 [52:04<20:09,  6.47s/it] 72%|███████▏  | 471/657 [52:11<20:09,  6.50s/it] 72%|███████▏  | 472/657 [52:21<23:34,  7.64s/it] 72%|███████▏  | 473/657 [52:27<22:23,  7.30s/it] 72%|███████▏  | 474/657 [52:36<23:43,  7.78s/it] 72%|███████▏  | 475/657 [52:44<23:46,  7.84s/it] 72%|███████▏  | 476/657 [52:50<21:26,  7.11s/it] 73%|███████▎  | 477/657 [52:55<19:32,  6.51s/it] 73%|███████▎  | 478/657 [53:02<19:59,  6.70s/it] 73%|███████▎  | 479/657 [53:12<22:53,  7.72s/it] 73%|███████▎  | 480/657 [53:19<22:03,  7.48s/it]                                                  73%|███████▎  | 480/657 [53:19<22:03,  7.48s/it] 73%|███████▎  | 481/657 [53:24<19:30,  6.65s/it] 73%|███████▎  | 482/657 [53:31<20:14,  6.94s/it] 74%|███████▎  | 483/657 [53:40<22:04,  7.61s/it] 74%|███████▎  | 484/657 [53:45<18:55,  6.56s/it] 74%|███████▍  | 485/657 [53:49<16:53,  5.89s/it] 74%|███████▍  | 486/657 [53:55<17:13,  6.04s/it] 74%|███████▍  | 487/657 [54:01<16:46,  5.92s/it] 74%|███████▍  | 488/657 [54:08<17:29,  6.21s/it] 74%|███████▍  | 489/657 [54:13<16:56,  6.05s/it] 75%|███████▍  | 490/657 [54:20<17:28,  6.28s/it]                                                  75%|███████▍  | 490/657 [54:20<17:28,  6.28s/it] 75%|███████▍  | 491/657 [54:28<18:53,  6.83s/it] 75%|███████▍  | 492/657 [54:35<18:58,  6.90s/it] 75%|███████▌  | 493/657 [54:42<18:24,  6.73s/it] 75%|███████▌  | 494/657 [54:48<18:05,  6.66s/it] 75%|███████▌  | 495/657 [54:52<15:56,  5.90s/it] 75%|███████▌  | 496/657 [54:57<14:34,  5.43s/it] 76%|███████▌  | 497/657 [55:05<16:31,  6.20s/it] 76%|███████▌  | 498/657 [55:12<17:28,  6.60s/it] 76%|███████▌  | 499/657 [55:19<17:40,  6.71s/it] 76%|███████▌  | 500/657 [55:26<17:35,  6.72s/it]                                                  76%|███████▌  | 500/657 [55:26<17:35,  6.72s/it][INFO|trainer.py:4117] 2024-12-14 11:55:35,768 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2024-12-14 11:55:35,768 >>   Num examples = 196
[INFO|trainer.py:4122] 2024-12-14 11:55:35,768 >>   Batch size = 1
{'loss': 0.8026, 'grad_norm': 17.792255401611328, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.05}
{'loss': 0.7161, 'grad_norm': 14.530587196350098, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.09}
{'loss': 0.7056, 'grad_norm': 11.93674373626709, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.14}
{'loss': 0.7001, 'grad_norm': 11.634726524353027, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.18}
{'loss': 0.7179, 'grad_norm': 14.827861785888672, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.23}
{'loss': 0.7476, 'grad_norm': 14.521685600280762, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.27}
{'loss': 0.7811, 'grad_norm': 14.642767906188965, 'learning_rate': 9.998869765883566e-05, 'epoch': 0.32}
{'loss': 0.7467, 'grad_norm': 14.114239692687988, 'learning_rate': 9.986160499534318e-05, 'epoch': 0.36}
{'loss': 0.7631, 'grad_norm': 13.157015800476074, 'learning_rate': 9.959365197965824e-05, 'epoch': 0.41}
{'loss': 0.7891, 'grad_norm': 14.201859474182129, 'learning_rate': 9.918559558613344e-05, 'epoch': 0.46}
{'loss': 0.8351, 'grad_norm': 14.254973411560059, 'learning_rate': 9.863858858486735e-05, 'epoch': 0.5}
{'loss': 0.7195, 'grad_norm': 11.239850997924805, 'learning_rate': 9.795417628509857e-05, 'epoch': 0.55}
{'loss': 0.7844, 'grad_norm': 12.586457252502441, 'learning_rate': 9.713429216966624e-05, 'epoch': 0.59}
{'loss': 0.7429, 'grad_norm': 13.004475593566895, 'learning_rate': 9.618125243286989e-05, 'epoch': 0.64}
{'loss': 0.6981, 'grad_norm': 12.460822105407715, 'learning_rate': 9.509774943715939e-05, 'epoch': 0.68}
{'loss': 0.7191, 'grad_norm': 13.311805725097656, 'learning_rate': 9.388684410713977e-05, 'epoch': 0.73}
{'loss': 0.6787, 'grad_norm': 9.66253662109375, 'learning_rate': 9.255195728237838e-05, 'epoch': 0.77}
{'loss': 0.7326, 'grad_norm': 13.64234447479248, 'learning_rate': 9.109686005344258e-05, 'epoch': 0.82}
{'loss': 0.7442, 'grad_norm': 10.677658081054688, 'learning_rate': 8.95256631084693e-05, 'epoch': 0.87}
{'loss': 0.7237, 'grad_norm': 12.779364585876465, 'learning_rate': 8.784280512036235e-05, 'epoch': 0.91}
{'loss': 0.6915, 'grad_norm': 11.416346549987793, 'learning_rate': 8.60530402074241e-05, 'epoch': 0.96}
{'loss': 0.7451, 'grad_norm': 15.03251838684082, 'learning_rate': 8.416142450284565e-05, 'epoch': 1.0}
{'loss': 0.8094, 'grad_norm': 12.220109939575195, 'learning_rate': 8.217330187099688e-05, 'epoch': 1.05}
{'loss': 0.7263, 'grad_norm': 13.004536628723145, 'learning_rate': 8.009428881086835e-05, 'epoch': 1.09}
{'loss': 0.6922, 'grad_norm': 14.287942886352539, 'learning_rate': 7.793025858931318e-05, 'epoch': 1.14}
{'loss': 0.7353, 'grad_norm': 14.2800931930542, 'learning_rate': 7.568732464891293e-05, 'epoch': 1.19}
{'loss': 0.7768, 'grad_norm': 13.40993595123291, 'learning_rate': 7.33718233373407e-05, 'epoch': 1.23}
{'loss': 0.7873, 'grad_norm': 13.94593620300293, 'learning_rate': 7.099029600701143e-05, 'epoch': 1.28}
{'loss': 0.7415, 'grad_norm': 11.718391418457031, 'learning_rate': 6.854947053558849e-05, 'epoch': 1.32}
{'loss': 0.6735, 'grad_norm': 11.61841869354248, 'learning_rate': 6.605624231955131e-05, 'epoch': 1.37}
{'loss': 0.7272, 'grad_norm': 12.23446273803711, 'learning_rate': 6.351765479451815e-05, 'epoch': 1.41}
{'loss': 0.749, 'grad_norm': 15.19927978515625, 'learning_rate': 6.094087953735423e-05, 'epoch': 1.46}
{'loss': 0.7535, 'grad_norm': 12.260248184204102, 'learning_rate': 5.833319600627753e-05, 'epoch': 1.5}
{'loss': 0.6728, 'grad_norm': 9.829938888549805, 'learning_rate': 5.570197097619688e-05, 'epoch': 1.55}
{'loss': 0.7616, 'grad_norm': 12.357400894165039, 'learning_rate': 5.305463772737812e-05, 'epoch': 1.6}
{'loss': 0.687, 'grad_norm': 12.600902557373047, 'learning_rate': 5.0398675046230835e-05, 'epoch': 1.64}
{'loss': 0.755, 'grad_norm': 11.520956039428711, 'learning_rate': 4.7741586097539075e-05, 'epoch': 1.69}
{'loss': 0.7475, 'grad_norm': 13.987581253051758, 'learning_rate': 4.509087722782242e-05, 'epoch': 1.73}
{'loss': 0.7485, 'grad_norm': 13.7508544921875, 'learning_rate': 4.2454036759708765e-05, 'epoch': 1.78}
{'loss': 0.7539, 'grad_norm': 13.038186073303223, 'learning_rate': 3.983851383722482e-05, 'epoch': 1.82}
{'loss': 0.7031, 'grad_norm': 12.167227745056152, 'learning_rate': 3.725169738176737e-05, 'epoch': 1.87}
{'loss': 0.6868, 'grad_norm': 12.324240684509277, 'learning_rate': 3.470089521820502e-05, 'epoch': 1.91}
{'loss': 0.7269, 'grad_norm': 12.196917533874512, 'learning_rate': 3.219331343007974e-05, 'epoch': 1.96}
{'loss': 0.7547, 'grad_norm': 12.952717781066895, 'learning_rate': 2.9736036002230332e-05, 'epoch': 2.01}
{'loss': 0.6896, 'grad_norm': 12.644866943359375, 'learning_rate': 2.7336004808348093e-05, 'epoch': 2.05}
{'loss': 0.7172, 'grad_norm': 14.354116439819336, 'learning_rate': 2.500000000000001e-05, 'epoch': 2.1}
{'loss': 0.677, 'grad_norm': 11.387285232543945, 'learning_rate': 2.273462085252146e-05, 'epoch': 2.14}
{'loss': 0.7113, 'grad_norm': 12.071154594421387, 'learning_rate': 2.054626712188886e-05, 'epoch': 2.19}
{'loss': 0.6976, 'grad_norm': 12.690001487731934, 'learning_rate': 1.8441120965239912e-05, 'epoch': 2.23}
{'loss': 0.7082, 'grad_norm': 13.157438278198242, 'learning_rate': 1.642512947611622e-05, 'epoch': 2.28}

  0%|          | 0/196 [00:00<?, ?it/s][A
  1%|          | 2/196 [00:00<00:15, 12.29it/s][A
  2%|▏         | 4/196 [00:00<00:35,  5.36it/s][A
  3%|▎         | 5/196 [00:00<00:34,  5.47it/s][A
  3%|▎         | 6/196 [00:00<00:31,  6.01it/s][A
  4%|▎         | 7/196 [00:01<00:28,  6.64it/s][A
  4%|▍         | 8/196 [00:01<00:26,  7.05it/s][A
  5%|▍         | 9/196 [00:01<00:40,  4.59it/s][A
  5%|▌         | 10/196 [00:02<00:51,  3.64it/s][A
  6%|▌         | 11/196 [00:02<00:47,  3.90it/s][A
  6%|▌         | 12/196 [00:02<00:51,  3.56it/s][A
  7%|▋         | 13/196 [00:02<00:49,  3.68it/s][A
  7%|▋         | 14/196 [00:03<00:47,  3.86it/s][A
  8%|▊         | 15/196 [00:03<00:48,  3.72it/s][A
  8%|▊         | 16/196 [00:03<00:51,  3.52it/s][A
  9%|▊         | 17/196 [00:04<00:53,  3.32it/s][A
  9%|▉         | 18/196 [00:04<00:48,  3.70it/s][A
 10%|▉         | 19/196 [00:04<00:44,  3.95it/s][A
 10%|█         | 20/196 [00:04<00:40,  4.37it/s][A
 11%|█         | 21/196 [00:04<00:33,  5.17it/s][A
 11%|█         | 22/196 [00:04<00:29,  5.86it/s][A
 12%|█▏        | 23/196 [00:04<00:26,  6.53it/s][A
 12%|█▏        | 24/196 [00:05<00:30,  5.71it/s][A
 13%|█▎        | 25/196 [00:05<00:29,  5.75it/s][A
 13%|█▎        | 26/196 [00:05<00:26,  6.31it/s][A
 14%|█▍        | 27/196 [00:05<00:30,  5.46it/s][A
 14%|█▍        | 28/196 [00:05<00:31,  5.39it/s][A
 15%|█▍        | 29/196 [00:06<00:34,  4.86it/s][A
 15%|█▌        | 30/196 [00:06<00:33,  5.01it/s][A
 16%|█▌        | 31/196 [00:06<00:28,  5.72it/s][A
 16%|█▋        | 32/196 [00:06<00:25,  6.34it/s][A
 17%|█▋        | 33/196 [00:06<00:36,  4.45it/s][A
 17%|█▋        | 34/196 [00:07<00:38,  4.17it/s][A
 18%|█▊        | 35/196 [00:07<00:32,  4.88it/s][A
 18%|█▊        | 36/196 [00:07<00:35,  4.55it/s][A
 19%|█▉        | 37/196 [00:07<00:34,  4.62it/s][A
 19%|█▉        | 38/196 [00:08<00:37,  4.27it/s][A
 20%|█▉        | 39/196 [00:08<00:35,  4.48it/s][A
 20%|██        | 40/196 [00:08<00:30,  5.19it/s][A
 21%|██        | 41/196 [00:08<00:31,  4.90it/s][A
 21%|██▏       | 42/196 [00:08<00:35,  4.31it/s][A
 22%|██▏       | 43/196 [00:09<00:39,  3.88it/s][A
 22%|██▏       | 44/196 [00:09<00:42,  3.58it/s][A
 23%|██▎       | 45/196 [00:09<00:38,  3.96it/s][A
 23%|██▎       | 46/196 [00:09<00:31,  4.80it/s][A
 24%|██▍       | 47/196 [00:09<00:26,  5.55it/s][A
 24%|██▍       | 48/196 [00:10<00:24,  6.05it/s][A
 25%|██▌       | 49/196 [00:10<00:22,  6.47it/s][A
 26%|██▌       | 50/196 [00:10<00:20,  7.05it/s][A
 26%|██▌       | 51/196 [00:10<00:24,  6.03it/s][A
 27%|██▋       | 52/196 [00:10<00:25,  5.71it/s][A
 27%|██▋       | 53/196 [00:10<00:23,  6.17it/s][A
 28%|██▊       | 54/196 [00:11<00:26,  5.34it/s][A
 28%|██▊       | 55/196 [00:11<00:30,  4.60it/s][A
 29%|██▊       | 56/196 [00:11<00:33,  4.18it/s][A
 29%|██▉       | 57/196 [00:12<00:37,  3.70it/s][A
 30%|██▉       | 58/196 [00:12<00:34,  4.02it/s][A
 30%|███       | 59/196 [00:12<00:33,  4.10it/s][A
 31%|███       | 60/196 [00:12<00:30,  4.43it/s][A
 31%|███       | 61/196 [00:12<00:32,  4.10it/s][A
 32%|███▏      | 62/196 [00:13<00:36,  3.67it/s][A
 32%|███▏      | 63/196 [00:13<00:32,  4.11it/s][A
 33%|███▎      | 64/196 [00:13<00:35,  3.75it/s][A
 33%|███▎      | 65/196 [00:14<00:38,  3.36it/s][A
 34%|███▎      | 66/196 [00:14<00:34,  3.78it/s][A
 34%|███▍      | 67/196 [00:14<00:28,  4.54it/s][A
 35%|███▍      | 68/196 [00:14<00:33,  3.85it/s][A
 35%|███▌      | 69/196 [00:15<00:38,  3.34it/s][A
 36%|███▌      | 70/196 [00:15<00:37,  3.33it/s][A
 36%|███▌      | 71/196 [00:15<00:37,  3.36it/s][A
 37%|███▋      | 72/196 [00:16<00:37,  3.32it/s][A
 37%|███▋      | 73/196 [00:16<00:41,  2.94it/s][A
 38%|███▊      | 74/196 [00:16<00:38,  3.17it/s][A
 38%|███▊      | 75/196 [00:16<00:30,  3.91it/s][A
 39%|███▉      | 76/196 [00:17<00:29,  4.00it/s][A
 39%|███▉      | 77/196 [00:17<00:27,  4.34it/s][A
 40%|███▉      | 78/196 [00:17<00:28,  4.10it/s][A
 40%|████      | 79/196 [00:17<00:27,  4.20it/s][A
 41%|████      | 80/196 [00:17<00:23,  4.85it/s][A
 41%|████▏     | 81/196 [00:18<00:21,  5.42it/s][A
 42%|████▏     | 82/196 [00:18<00:23,  4.92it/s][A
 42%|████▏     | 83/196 [00:18<00:22,  5.06it/s][A
 43%|████▎     | 84/196 [00:18<00:27,  4.11it/s][A
 43%|████▎     | 85/196 [00:19<00:31,  3.49it/s][A
 44%|████▍     | 86/196 [00:19<00:32,  3.37it/s][A
 44%|████▍     | 87/196 [00:19<00:28,  3.80it/s][A
 45%|████▍     | 88/196 [00:19<00:23,  4.50it/s][A
 45%|████▌     | 89/196 [00:20<00:20,  5.25it/s][A
 46%|████▌     | 90/196 [00:20<00:26,  3.99it/s][A
 46%|████▋     | 91/196 [00:20<00:31,  3.28it/s][A
 47%|████▋     | 92/196 [00:21<00:34,  3.06it/s][A
 47%|████▋     | 93/196 [00:21<00:30,  3.42it/s][A
 48%|████▊     | 94/196 [00:21<00:24,  4.16it/s][A
 48%|████▊     | 95/196 [00:21<00:20,  4.96it/s][A
 49%|████▉     | 96/196 [00:21<00:17,  5.69it/s][A
 49%|████▉     | 97/196 [00:21<00:15,  6.22it/s][A
 50%|█████     | 98/196 [00:22<00:14,  6.85it/s][A
 51%|█████     | 99/196 [00:22<00:13,  7.16it/s][A
 51%|█████     | 100/196 [00:22<00:13,  7.31it/s][A
 52%|█████▏    | 101/196 [00:22<00:12,  7.76it/s][A
 52%|█████▏    | 102/196 [00:22<00:11,  8.00it/s][A
 53%|█████▎    | 103/196 [00:22<00:11,  8.24it/s][A
 53%|█████▎    | 104/196 [00:22<00:10,  8.49it/s][A
 54%|█████▎    | 105/196 [00:23<00:17,  5.34it/s][A
 54%|█████▍    | 106/196 [00:23<00:18,  4.77it/s][A
 55%|█████▍    | 107/196 [00:23<00:19,  4.52it/s][A
 55%|█████▌    | 108/196 [00:23<00:22,  3.97it/s][A
 56%|█████▌    | 109/196 [00:24<00:20,  4.20it/s][A
 56%|█████▌    | 110/196 [00:24<00:23,  3.65it/s][A
 57%|█████▋    | 111/196 [00:24<00:23,  3.70it/s][A
 57%|█████▋    | 112/196 [00:25<00:25,  3.33it/s][A
 58%|█████▊    | 113/196 [00:25<00:27,  3.07it/s][A
 58%|█████▊    | 114/196 [00:25<00:26,  3.12it/s][A
 59%|█████▊    | 115/196 [00:26<00:22,  3.53it/s][A
 59%|█████▉    | 116/196 [00:26<00:18,  4.22it/s][A
 60%|█████▉    | 117/196 [00:26<00:19,  3.98it/s][A
 60%|██████    | 118/196 [00:26<00:18,  4.18it/s][A
 61%|██████    | 119/196 [00:27<00:21,  3.52it/s][A
 61%|██████    | 120/196 [00:27<00:21,  3.59it/s][A
 62%|██████▏   | 121/196 [00:27<00:19,  3.80it/s][A
 62%|██████▏   | 122/196 [00:27<00:21,  3.41it/s][A
 63%|██████▎   | 123/196 [00:28<00:20,  3.61it/s][A
 63%|██████▎   | 124/196 [00:28<00:16,  4.31it/s][A
 64%|██████▍   | 125/196 [00:28<00:13,  5.10it/s][A
 64%|██████▍   | 126/196 [00:28<00:12,  5.75it/s][A
 65%|██████▍   | 127/196 [00:28<00:13,  5.14it/s][A
 65%|██████▌   | 128/196 [00:29<00:15,  4.40it/s][A
 66%|██████▌   | 129/196 [00:29<00:17,  3.83it/s][A
 66%|██████▋   | 130/196 [00:29<00:16,  4.04it/s][A
 67%|██████▋   | 131/196 [00:29<00:13,  4.84it/s][A
 67%|██████▋   | 132/196 [00:29<00:11,  5.70it/s][A
 68%|██████▊   | 133/196 [00:29<00:09,  6.34it/s][A
 68%|██████▊   | 134/196 [00:30<00:09,  6.72it/s][A
 69%|██████▉   | 135/196 [00:30<00:11,  5.20it/s][A
 69%|██████▉   | 136/196 [00:30<00:14,  4.28it/s][A
 70%|██████▉   | 137/196 [00:30<00:12,  4.56it/s][A
 70%|███████   | 138/196 [00:30<00:11,  5.21it/s][A
 71%|███████   | 139/196 [00:31<00:12,  4.40it/s][A
 71%|███████▏  | 140/196 [00:31<00:12,  4.43it/s][A
 72%|███████▏  | 141/196 [00:31<00:12,  4.37it/s][A
 72%|███████▏  | 142/196 [00:31<00:11,  4.57it/s][A
 73%|███████▎  | 143/196 [00:32<00:11,  4.54it/s][A
 73%|███████▎  | 144/196 [00:32<00:10,  4.94it/s][A
 74%|███████▍  | 145/196 [00:32<00:10,  4.72it/s][A
 74%|███████▍  | 146/196 [00:32<00:10,  4.89it/s][A
 75%|███████▌  | 147/196 [00:32<00:08,  5.51it/s][A
 76%|███████▌  | 148/196 [00:33<00:09,  5.25it/s][A
 76%|███████▌  | 149/196 [00:33<00:11,  4.20it/s][A
 77%|███████▋  | 150/196 [00:33<00:12,  3.67it/s][A
 77%|███████▋  | 151/196 [00:34<00:12,  3.57it/s][A
 78%|███████▊  | 152/196 [00:34<00:14,  3.04it/s][A
 78%|███████▊  | 153/196 [00:34<00:13,  3.22it/s][A
 79%|███████▊  | 154/196 [00:34<00:10,  3.94it/s][A
 79%|███████▉  | 155/196 [00:35<00:09,  4.31it/s][A
 80%|███████▉  | 156/196 [00:35<00:10,  3.88it/s][A
 80%|████████  | 157/196 [00:35<00:12,  3.11it/s][A
 81%|████████  | 158/196 [00:36<00:13,  2.83it/s][A
 81%|████████  | 159/196 [00:36<00:12,  2.92it/s][A
 82%|████████▏ | 160/196 [00:36<00:10,  3.42it/s][A
 82%|████████▏ | 161/196 [00:36<00:08,  4.17it/s][A
 83%|████████▎ | 162/196 [00:37<00:09,  3.65it/s][A
 83%|████████▎ | 163/196 [00:37<00:08,  3.73it/s][A
 84%|████████▎ | 164/196 [00:37<00:07,  4.42it/s][A
 84%|████████▍ | 165/196 [00:37<00:07,  4.08it/s][A
 85%|████████▍ | 166/196 [00:38<00:07,  4.20it/s][A
 85%|████████▌ | 167/196 [00:38<00:05,  4.86it/s][A
 86%|████████▌ | 168/196 [00:38<00:06,  4.59it/s][A
 86%|████████▌ | 169/196 [00:38<00:06,  4.08it/s][A
 87%|████████▋ | 170/196 [00:39<00:06,  3.77it/s][A
 87%|████████▋ | 171/196 [00:39<00:05,  4.20it/s][A
 88%|████████▊ | 172/196 [00:39<00:04,  5.05it/s][A
 88%|████████▊ | 173/196 [00:39<00:04,  5.72it/s][A
 89%|████████▉ | 174/196 [00:39<00:05,  4.18it/s][A
 89%|████████▉ | 175/196 [00:40<00:05,  3.50it/s][A
 90%|████████▉ | 176/196 [00:40<00:05,  3.86it/s][A
 90%|█████████ | 177/196 [00:40<00:04,  4.57it/s][A
 91%|█████████ | 178/196 [00:40<00:03,  5.15it/s][A
 91%|█████████▏| 179/196 [00:41<00:03,  4.29it/s][A
 92%|█████████▏| 180/196 [00:41<00:03,  4.33it/s][A
 92%|█████████▏| 181/196 [00:41<00:02,  5.08it/s][A
 93%|█████████▎| 182/196 [00:41<00:02,  5.60it/s][A
 93%|█████████▎| 183/196 [00:41<00:03,  4.28it/s][A
 94%|█████████▍| 184/196 [00:42<00:02,  4.15it/s][A
 94%|█████████▍| 185/196 [00:42<00:02,  4.25it/s][A
 95%|█████████▍| 186/196 [00:42<00:02,  4.55it/s][A
 95%|█████████▌| 187/196 [00:43<00:02,  3.68it/s][A
 96%|█████████▌| 188/196 [00:43<00:02,  3.19it/s][A
 96%|█████████▋| 189/196 [00:43<00:02,  2.85it/s][A
 97%|█████████▋| 190/196 [00:44<00:02,  2.68it/s][A
 97%|█████████▋| 191/196 [00:44<00:02,  2.45it/s][A
 98%|█████████▊| 192/196 [00:45<00:01,  2.46it/s][A
 98%|█████████▊| 193/196 [00:45<00:01,  2.93it/s][A
 99%|█████████▉| 194/196 [00:45<00:00,  3.24it/s][A
 99%|█████████▉| 195/196 [00:45<00:00,  3.27it/s][A
100%|██████████| 196/196 [00:46<00:00,  3.77it/s][A                                                 
                                                 [A 76%|███████▌  | 500/657 [56:12<17:35,  6.72s/it]
100%|██████████| 196/196 [00:46<00:00,  3.77it/s][A
                                                 [A[INFO|trainer.py:3801] 2024-12-14 11:56:22,080 >> Saving model checkpoint to /data/fxy/ecpo/fine-tuning/reward/checkpoint-500
[INFO|trainer.py:3815] 2024-12-14 11:56:22,081 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2646] 2024-12-14 11:56:22,084 >> tokenizer config file saved in /data/fxy/ecpo/fine-tuning/reward/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2024-12-14 11:56:22,084 >> Special tokens file saved in /data/fxy/ecpo/fine-tuning/reward/checkpoint-500/special_tokens_map.json
[INFO|configuration_utils.py:677] 2024-12-14 11:56:22,476 >> loading configuration file /data/pretrain_dir/Llama-3.1/config.json
[INFO|configuration_utils.py:746] 2024-12-14 11:56:22,477 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

{'eval_loss': 0.694945216178894, 'eval_accuracy': 0.46938775510204084, 'eval_runtime': 46.3111, 'eval_samples_per_second': 4.232, 'eval_steps_per_second': 4.232, 'epoch': 2.28}
[INFO|2024-12-14 11:56:22] llamafactory.train.callbacks:157 >> Value head model saved at: /data/fxy/ecpo/fine-tuning/reward/checkpoint-500
/data/fxy/anaconda3/envs/ecpo/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
 76%|███████▋  | 501/657 [56:19<53:44, 20.67s/it] 76%|███████▋  | 502/657 [56:29<45:08, 17.47s/it] 77%|███████▋  | 503/657 [56:34<34:49, 13.57s/it] 77%|███████▋  | 504/657 [56:41<29:32, 11.58s/it] 77%|███████▋  | 505/657 [56:48<25:55, 10.23s/it] 77%|███████▋  | 506/657 [56:55<23:53,  9.49s/it] 77%|███████▋  | 507/657 [57:05<23:53,  9.56s/it] 77%|███████▋  | 508/657 [57:12<21:39,  8.72s/it] 77%|███████▋  | 509/657 [57:20<20:52,  8.46s/it] 78%|███████▊  | 510/657 [57:28<20:40,  8.44s/it]                                                  78%|███████▊  | 510/657 [57:28<20:40,  8.44s/it] 78%|███████▊  | 511/657 [57:35<19:19,  7.94s/it] 78%|███████▊  | 512/657 [57:42<18:37,  7.71s/it] 78%|███████▊  | 513/657 [57:49<18:15,  7.61s/it] 78%|███████▊  | 514/657 [57:55<16:57,  7.12s/it] 78%|███████▊  | 515/657 [58:01<16:02,  6.78s/it] 79%|███████▊  | 516/657 [58:08<16:01,  6.82s/it] 79%|███████▊  | 517/657 [58:16<16:08,  6.92s/it] 79%|███████▉  | 518/657 [58:23<16:34,  7.15s/it] 79%|███████▉  | 519/657 [58:27<13:49,  6.01s/it] 79%|███████▉  | 520/657 [58:34<14:54,  6.53s/it]                                                  79%|███████▉  | 520/657 [58:34<14:54,  6.53s/it] 79%|███████▉  | 521/657 [58:41<14:46,  6.52s/it] 79%|███████▉  | 522/657 [58:47<14:15,  6.34s/it] 80%|███████▉  | 523/657 [58:57<16:42,  7.48s/it] 80%|███████▉  | 524/657 [59:02<14:47,  6.67s/it] 80%|███████▉  | 525/657 [59:10<15:44,  7.16s/it] 80%|████████  | 526/657 [59:16<14:46,  6.77s/it] 80%|████████  | 527/657 [59:21<13:36,  6.28s/it] 80%|████████  | 528/657 [59:27<13:35,  6.32s/it] 81%|████████  | 529/657 [59:34<13:46,  6.46s/it] 81%|████████  | 530/657 [59:41<13:58,  6.61s/it]                                                  81%|████████  | 530/657 [59:41<13:58,  6.61s/it] 81%|████████  | 531/657 [59:49<14:35,  6.95s/it] 81%|████████  | 532/657 [59:56<14:18,  6.87s/it] 81%|████████  | 533/657 [1:00:02<13:52,  6.71s/it] 81%|████████▏ | 534/657 [1:00:07<12:44,  6.21s/it] 81%|████████▏ | 535/657 [1:00:14<13:22,  6.58s/it] 82%|████████▏ | 536/657 [1:00:22<13:54,  6.90s/it] 82%|████████▏ | 537/657 [1:00:30<14:30,  7.25s/it] 82%|████████▏ | 538/657 [1:00:38<14:50,  7.48s/it] 82%|████████▏ | 539/657 [1:00:46<15:05,  7.67s/it] 82%|████████▏ | 540/657 [1:00:54<15:01,  7.70s/it]                                                    82%|████████▏ | 540/657 [1:00:54<15:01,  7.70s/it] 82%|████████▏ | 541/657 [1:01:02<15:08,  7.83s/it] 82%|████████▏ | 542/657 [1:01:10<15:19,  7.99s/it] 83%|████████▎ | 543/657 [1:01:20<15:47,  8.31s/it] 83%|████████▎ | 544/657 [1:01:27<14:55,  7.92s/it] 83%|████████▎ | 545/657 [1:01:34<14:35,  7.81s/it] 83%|████████▎ | 546/657 [1:01:41<14:10,  7.66s/it] 83%|████████▎ | 547/657 [1:01:48<13:12,  7.20s/it] 83%|████████▎ | 548/657 [1:01:54<12:33,  6.91s/it] 84%|████████▎ | 549/657 [1:01:59<11:41,  6.49s/it] 84%|████████▎ | 550/657 [1:02:06<11:47,  6.62s/it]                                                    84%|████████▎ | 550/657 [1:02:06<11:47,  6.62s/it] 84%|████████▍ | 551/657 [1:02:13<11:31,  6.53s/it] 84%|████████▍ | 552/657 [1:02:18<11:05,  6.34s/it] 84%|████████▍ | 553/657 [1:02:23<10:07,  5.84s/it] 84%|████████▍ | 554/657 [1:02:31<11:09,  6.50s/it] 84%|████████▍ | 555/657 [1:02:37<10:36,  6.24s/it] 85%|████████▍ | 556/657 [1:02:43<10:34,  6.28s/it] 85%|████████▍ | 557/657 [1:02:51<11:10,  6.71s/it] 85%|████████▍ | 558/657 [1:02:58<11:22,  6.89s/it] 85%|████████▌ | 559/657 [1:03:03<10:14,  6.27s/it] 85%|████████▌ | 560/657 [1:03:08<09:44,  6.03s/it]                                                    85%|████████▌ | 560/657 [1:03:08<09:44,  6.03s/it] 85%|████████▌ | 561/657 [1:03:15<09:58,  6.24s/it] 86%|████████▌ | 562/657 [1:03:20<09:14,  5.84s/it] 86%|████████▌ | 563/657 [1:03:26<09:22,  5.98s/it] 86%|████████▌ | 564/657 [1:03:32<09:01,  5.82s/it] 86%|████████▌ | 565/657 [1:03:38<09:09,  5.97s/it] 86%|████████▌ | 566/657 [1:03:44<08:52,  5.85s/it] 86%|████████▋ | 567/657 [1:03:50<08:49,  5.88s/it] 86%|████████▋ | 568/657 [1:03:58<09:35,  6.47s/it] 87%|████████▋ | 569/657 [1:04:05<09:53,  6.75s/it] 87%|████████▋ | 570/657 [1:04:11<09:34,  6.61s/it]                                                    87%|████████▋ | 570/657 [1:04:11<09:34,  6.61s/it] 87%|████████▋ | 571/657 [1:04:15<08:15,  5.76s/it] 87%|████████▋ | 572/657 [1:04:22<08:48,  6.22s/it] 87%|████████▋ | 573/657 [1:04:31<09:50,  7.03s/it] 87%|████████▋ | 574/657 [1:04:39<09:54,  7.17s/it] 88%|████████▊ | 575/657 [1:04:45<09:37,  7.05s/it] 88%|████████▊ | 576/657 [1:04:52<09:16,  6.87s/it] 88%|████████▊ | 577/657 [1:05:00<09:32,  7.16s/it] 88%|████████▊ | 578/657 [1:05:07<09:16,  7.04s/it] 88%|████████▊ | 579/657 [1:05:11<08:10,  6.29s/it] 88%|████████▊ | 580/657 [1:05:17<07:54,  6.16s/it]                                                    88%|████████▊ | 580/657 [1:05:17<07:54,  6.16s/it] 88%|████████▊ | 581/657 [1:05:23<07:43,  6.09s/it] 89%|████████▊ | 582/657 [1:05:27<06:52,  5.50s/it] 89%|████████▊ | 583/657 [1:05:33<07:05,  5.75s/it] 89%|████████▉ | 584/657 [1:05:39<06:49,  5.61s/it] 89%|████████▉ | 585/657 [1:05:43<06:09,  5.13s/it] 89%|████████▉ | 586/657 [1:05:48<06:18,  5.33s/it] 89%|████████▉ | 587/657 [1:05:56<07:09,  6.14s/it] 89%|████████▉ | 588/657 [1:06:03<07:11,  6.26s/it] 90%|████████▉ | 589/657 [1:06:10<07:27,  6.58s/it] 90%|████████▉ | 590/657 [1:06:17<07:16,  6.51s/it]                                                    90%|████████▉ | 590/657 [1:06:17<07:16,  6.51s/it] 90%|████████▉ | 591/657 [1:06:22<06:50,  6.22s/it] 90%|█████████ | 592/657 [1:06:30<07:06,  6.56s/it] 90%|█████████ | 593/657 [1:06:35<06:33,  6.16s/it] 90%|█████████ | 594/657 [1:06:39<05:44,  5.47s/it] 91%|█████████ | 595/657 [1:06:44<05:42,  5.53s/it] 91%|█████████ | 596/657 [1:06:53<06:32,  6.43s/it] 91%|█████████ | 597/657 [1:06:59<06:20,  6.34s/it] 91%|█████████ | 598/657 [1:07:04<05:54,  6.01s/it] 91%|█████████ | 599/657 [1:07:10<05:48,  6.01s/it] 91%|█████████▏| 600/657 [1:07:18<06:16,  6.60s/it]                                                    91%|█████████▏| 600/657 [1:07:18<06:16,  6.60s/it] 91%|█████████▏| 601/657 [1:07:23<05:39,  6.06s/it] 92%|█████████▏| 602/657 [1:07:32<06:16,  6.84s/it] 92%|█████████▏| 603/657 [1:07:37<05:52,  6.53s/it] 92%|█████████▏| 604/657 [1:07:44<05:41,  6.45s/it] 92%|█████████▏| 605/657 [1:07:49<05:25,  6.25s/it] 92%|█████████▏| 606/657 [1:07:55<05:14,  6.16s/it] 92%|█████████▏| 607/657 [1:08:02<05:18,  6.37s/it] 93%|█████████▎| 608/657 [1:08:08<05:08,  6.30s/it] 93%|█████████▎| 609/657 [1:08:14<04:56,  6.18s/it] 93%|█████████▎| 610/657 [1:08:21<04:50,  6.19s/it]                                                    93%|█████████▎| 610/657 [1:08:21<04:50,  6.19s/it] 93%|█████████▎| 611/657 [1:08:28<04:56,  6.44s/it] 93%|█████████▎| 612/657 [1:08:33<04:41,  6.26s/it] 93%|█████████▎| 613/657 [1:08:42<05:06,  6.97s/it] 93%|█████████▎| 614/657 [1:08:48<04:45,  6.64s/it] 94%|█████████▎| 615/657 [1:08:56<04:57,  7.08s/it] 94%|█████████▍| 616/657 [1:09:02<04:39,  6.82s/it] 94%|█████████▍| 617/657 [1:09:09<04:38,  6.96s/it] 94%|█████████▍| 618/657 [1:09:16<04:26,  6.83s/it] 94%|█████████▍| 619/657 [1:09:24<04:34,  7.23s/it] 94%|█████████▍| 620/657 [1:09:30<04:13,  6.84s/it]                                                    94%|█████████▍| 620/657 [1:09:30<04:13,  6.84s/it] 95%|█████████▍| 621/657 [1:09:35<03:40,  6.13s/it] 95%|█████████▍| 622/657 [1:09:44<04:03,  6.96s/it] 95%|█████████▍| 623/657 [1:09:51<03:58,  7.01s/it] 95%|█████████▍| 624/657 [1:09:58<03:56,  7.17s/it] 95%|█████████▌| 625/657 [1:10:01<03:10,  5.94s/it] 95%|█████████▌| 626/657 [1:10:09<03:20,  6.46s/it] 95%|█████████▌| 627/657 [1:10:17<03:24,  6.83s/it] 96%|█████████▌| 628/657 [1:10:24<03:20,  6.92s/it] 96%|█████████▌| 629/657 [1:10:31<03:12,  6.88s/it] 96%|█████████▌| 630/657 [1:10:37<03:03,  6.80s/it]                                                    96%|█████████▌| 630/657 [1:10:37<03:03,  6.80s/it] 96%|█████████▌| 631/657 [1:10:41<02:31,  5.82s/it] 96%|█████████▌| 632/657 [1:10:49<02:41,  6.45s/it] 96%|█████████▋| 633/657 [1:10:57<02:46,  6.95s/it] 96%|█████████▋| 634/657 [1:11:02<02:25,  6.34s/it] 97%|█████████▋| 635/657 [1:11:09<02:27,  6.70s/it] 97%|█████████▋| 636/657 [1:11:17<02:29,  7.12s/it] 97%|█████████▋| 637/657 [1:11:24<02:19,  6.99s/it] 97%|█████████▋| 638/657 [1:11:30<02:09,  6.81s/it] 97%|█████████▋| 639/657 [1:11:38<02:04,  6.94s/it] 97%|█████████▋| 640/657 [1:11:46<02:04,  7.32s/it]                                                    97%|█████████▋| 640/657 [1:11:46<02:04,  7.32s/it] 98%|█████████▊| 641/657 [1:11:51<01:44,  6.55s/it] 98%|█████████▊| 642/657 [1:11:59<01:44,  6.99s/it] 98%|█████████▊| 643/657 [1:12:04<01:30,  6.48s/it] 98%|█████████▊| 644/657 [1:12:11<01:25,  6.61s/it] 98%|█████████▊| 645/657 [1:12:17<01:18,  6.56s/it] 98%|█████████▊| 646/657 [1:12:23<01:08,  6.26s/it] 98%|█████████▊| 647/657 [1:12:31<01:08,  6.81s/it] 99%|█████████▊| 648/657 [1:12:38<01:01,  6.78s/it] 99%|█████████▉| 649/657 [1:12:43<00:49,  6.22s/it] 99%|█████████▉| 650/657 [1:12:49<00:43,  6.23s/it]                                                    99%|█████████▉| 650/657 [1:12:49<00:43,  6.23s/it] 99%|█████████▉| 651/657 [1:12:57<00:40,  6.75s/it] 99%|█████████▉| 652/657 [1:13:03<00:33,  6.70s/it] 99%|█████████▉| 653/657 [1:13:09<00:26,  6.51s/it]100%|█████████▉| 654/657 [1:13:15<00:18,  6.30s/it]100%|█████████▉| 655/657 [1:13:21<00:12,  6.08s/it]100%|█████████▉| 656/657 [1:13:26<00:05,  5.94s/it]100%|██████████| 657/657 [1:13:35<00:00,  6.77s/it][INFO|trainer.py:3801] 2024-12-14 12:13:44,891 >> Saving model checkpoint to /data/fxy/ecpo/fine-tuning/reward/checkpoint-657
[INFO|trainer.py:3815] 2024-12-14 12:13:44,891 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2646] 2024-12-14 12:13:44,895 >> tokenizer config file saved in /data/fxy/ecpo/fine-tuning/reward/checkpoint-657/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2024-12-14 12:13:44,895 >> Special tokens file saved in /data/fxy/ecpo/fine-tuning/reward/checkpoint-657/special_tokens_map.json
[INFO|configuration_utils.py:677] 2024-12-14 12:13:45,301 >> loading configuration file /data/pretrain_dir/Llama-3.1/config.json
[INFO|configuration_utils.py:746] 2024-12-14 12:13:45,302 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

{'loss': 0.6962, 'grad_norm': 12.443536758422852, 'learning_rate': 1.4503987883766857e-05, 'epoch': 2.32}
{'loss': 0.7443, 'grad_norm': 10.510430335998535, 'learning_rate': 1.2683123463975143e-05, 'epoch': 2.37}
{'loss': 0.6766, 'grad_norm': 11.730457305908203, 'learning_rate': 1.0967680206861197e-05, 'epoch': 2.42}
{'loss': 0.7237, 'grad_norm': 11.812721252441406, 'learning_rate': 9.362504284973683e-06, 'epoch': 2.46}
{'loss': 0.6815, 'grad_norm': 11.655226707458496, 'learning_rate': 7.872130362724422e-06, 'epoch': 2.51}
{'loss': 0.7296, 'grad_norm': 12.257316589355469, 'learning_rate': 6.500768785841483e-06, 'epoch': 2.55}
{'loss': 0.694, 'grad_norm': 11.130090713500977, 'learning_rate': 5.2522936870311955e-06, 'epoch': 2.6}
{'loss': 0.7037, 'grad_norm': 12.70231819152832, 'learning_rate': 4.130232041450866e-06, 'epoch': 2.64}
{'loss': 0.7338, 'grad_norm': 11.483650207519531, 'learning_rate': 3.1377537029107174e-06, 'epoch': 2.69}
{'loss': 0.7128, 'grad_norm': 10.895042419433594, 'learning_rate': 2.277662448953066e-06, 'epoch': 2.74}
{'loss': 0.7022, 'grad_norm': 13.104287147521973, 'learning_rate': 1.5523880601066798e-06, 'epoch': 2.78}
{'loss': 0.7526, 'grad_norm': 11.290060043334961, 'learning_rate': 9.639794556925042e-07, 'epoch': 2.83}
{'loss': 0.7303, 'grad_norm': 14.260954856872559, 'learning_rate': 5.140989055724687e-07, 'epoch': 2.87}
{'loss': 0.7122, 'grad_norm': 11.276589393615723, 'learning_rate': 2.0401733419315727e-07, 'epoch': 2.92}
{'loss': 0.7534, 'grad_norm': 12.988402366638184, 'learning_rate': 3.461073019064842e-08, 'epoch': 2.96}
[INFO|2024-12-14 12:13:45] llamafactory.train.callbacks:157 >> Value head model saved at: /data/fxy/ecpo/fine-tuning/reward/checkpoint-657
[INFO|trainer.py:2584] 2024-12-14 12:13:45,397 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 657/657 [1:13:36<00:00,  6.77s/it]100%|██████████| 657/657 [1:13:36<00:00,  6.72s/it]
[INFO|trainer.py:3801] 2024-12-14 12:13:45,399 >> Saving model checkpoint to /data/fxy/ecpo/fine-tuning/reward
[INFO|trainer.py:3815] 2024-12-14 12:13:45,400 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|tokenization_utils_base.py:2646] 2024-12-14 12:13:45,402 >> tokenizer config file saved in /data/fxy/ecpo/fine-tuning/reward/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2024-12-14 12:13:45,403 >> Special tokens file saved in /data/fxy/ecpo/fine-tuning/reward/special_tokens_map.json
[INFO|configuration_utils.py:677] 2024-12-14 12:13:45,584 >> loading configuration file /data/pretrain_dir/Llama-3.1/config.json
[INFO|configuration_utils.py:746] 2024-12-14 12:13:45,585 >> Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": [
    128001,
    128008,
    128009
  ],
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 131072,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "high_freq_factor": 4.0,
    "low_freq_factor": 1.0,
    "original_max_position_embeddings": 8192,
    "rope_type": "llama3"
  },
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "vocab_size": 128256
}

{'train_runtime': 4416.0953, 'train_samples_per_second': 1.192, 'train_steps_per_second': 0.149, 'train_loss': 0.7289106457563659, 'epoch': 2.99}
[INFO|2024-12-14 12:13:45] llamafactory.train.callbacks:157 >> Value head model saved at: /data/fxy/ecpo/fine-tuning/reward
[INFO|trainer.py:4117] 2024-12-14 12:13:45,982 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2024-12-14 12:13:45,982 >>   Num examples = 196
[INFO|trainer.py:4122] 2024-12-14 12:13:45,982 >>   Batch size = 1
***** train metrics *****
  epoch                    =     2.9949
  total_flos               =        0GF
  train_loss               =     0.7289
  train_runtime            = 1:13:36.09
  train_samples_per_second =      1.192
  train_steps_per_second   =      0.149
Figure saved at: /data/fxy/ecpo/fine-tuning/reward/training_loss.png
Figure saved at: /data/fxy/ecpo/fine-tuning/reward/training_eval_loss.png
Figure saved at: /data/fxy/ecpo/fine-tuning/reward/training_eval_accuracy.png
  0%|          | 0/196 [00:00<?, ?it/s]  1%|          | 2/196 [00:00<00:15, 12.32it/s]  2%|▏         | 4/196 [00:00<00:35,  5.40it/s]  3%|▎         | 5/196 [00:00<00:34,  5.53it/s]  3%|▎         | 6/196 [00:00<00:31,  6.06it/s]  4%|▎         | 7/196 [00:01<00:28,  6.70it/s]  4%|▍         | 8/196 [00:01<00:26,  7.11it/s]  5%|▍         | 9/196 [00:01<00:40,  4.62it/s]  5%|▌         | 10/196 [00:02<00:50,  3.65it/s]  6%|▌         | 11/196 [00:02<00:47,  3.91it/s]  6%|▌         | 12/196 [00:02<00:51,  3.57it/s]  7%|▋         | 13/196 [00:02<00:49,  3.69it/s]  7%|▋         | 14/196 [00:03<00:46,  3.88it/s]  8%|▊         | 15/196 [00:03<00:48,  3.72it/s]  8%|▊         | 16/196 [00:03<00:51,  3.53it/s]  9%|▊         | 17/196 [00:03<00:53,  3.32it/s]  9%|▉         | 18/196 [00:04<00:48,  3.70it/s] 10%|▉         | 19/196 [00:04<00:44,  3.95it/s] 10%|█         | 20/196 [00:04<00:40,  4.38it/s] 11%|█         | 21/196 [00:04<00:33,  5.18it/s] 11%|█         | 22/196 [00:04<00:29,  5.88it/s] 12%|█▏        | 23/196 [00:04<00:26,  6.55it/s] 12%|█▏        | 24/196 [00:05<00:30,  5.72it/s] 13%|█▎        | 25/196 [00:05<00:29,  5.75it/s] 13%|█▎        | 26/196 [00:05<00:26,  6.32it/s] 14%|█▍        | 27/196 [00:05<00:30,  5.46it/s] 14%|█▍        | 28/196 [00:05<00:31,  5.40it/s] 15%|█▍        | 29/196 [00:06<00:34,  4.87it/s] 15%|█▌        | 30/196 [00:06<00:33,  5.01it/s] 16%|█▌        | 31/196 [00:06<00:28,  5.73it/s] 16%|█▋        | 32/196 [00:06<00:25,  6.37it/s] 17%|█▋        | 33/196 [00:06<00:36,  4.46it/s] 17%|█▋        | 34/196 [00:07<00:38,  4.18it/s] 18%|█▊        | 35/196 [00:07<00:32,  4.89it/s] 18%|█▊        | 36/196 [00:07<00:35,  4.56it/s] 19%|█▉        | 37/196 [00:07<00:34,  4.62it/s] 19%|█▉        | 38/196 [00:08<00:37,  4.27it/s] 20%|█▉        | 39/196 [00:08<00:35,  4.47it/s] 20%|██        | 40/196 [00:08<00:30,  5.19it/s] 21%|██        | 41/196 [00:08<00:31,  4.89it/s] 21%|██▏       | 42/196 [00:08<00:35,  4.30it/s] 22%|██▏       | 43/196 [00:09<00:39,  3.88it/s] 22%|██▏       | 44/196 [00:09<00:42,  3.57it/s] 23%|██▎       | 45/196 [00:09<00:38,  3.97it/s] 23%|██▎       | 46/196 [00:09<00:31,  4.81it/s] 24%|██▍       | 47/196 [00:09<00:26,  5.56it/s] 24%|██▍       | 48/196 [00:10<00:24,  6.05it/s] 25%|██▌       | 49/196 [00:10<00:22,  6.47it/s] 26%|██▌       | 50/196 [00:10<00:20,  7.03it/s] 26%|██▌       | 51/196 [00:10<00:24,  6.01it/s] 27%|██▋       | 52/196 [00:10<00:25,  5.70it/s] 27%|██▋       | 53/196 [00:10<00:23,  6.17it/s] 28%|██▊       | 54/196 [00:11<00:26,  5.34it/s] 28%|██▊       | 55/196 [00:11<00:30,  4.61it/s] 29%|██▊       | 56/196 [00:11<00:33,  4.17it/s] 29%|██▉       | 57/196 [00:12<00:37,  3.70it/s] 30%|██▉       | 58/196 [00:12<00:34,  4.02it/s] 30%|███       | 59/196 [00:12<00:33,  4.09it/s] 31%|███       | 60/196 [00:12<00:30,  4.42it/s] 31%|███       | 61/196 [00:12<00:32,  4.10it/s] 32%|███▏      | 62/196 [00:13<00:36,  3.67it/s] 32%|███▏      | 63/196 [00:13<00:32,  4.11it/s] 33%|███▎      | 64/196 [00:13<00:35,  3.75it/s] 33%|███▎      | 65/196 [00:14<00:38,  3.37it/s] 34%|███▎      | 66/196 [00:14<00:34,  3.78it/s] 34%|███▍      | 67/196 [00:14<00:28,  4.56it/s] 35%|███▍      | 68/196 [00:14<00:33,  3.86it/s] 35%|███▌      | 69/196 [00:15<00:37,  3.35it/s] 36%|███▌      | 70/196 [00:15<00:37,  3.34it/s] 36%|███▌      | 71/196 [00:15<00:37,  3.36it/s] 37%|███▋      | 72/196 [00:16<00:37,  3.32it/s] 37%|███▋      | 73/196 [00:16<00:41,  2.95it/s] 38%|███▊      | 74/196 [00:16<00:38,  3.18it/s] 38%|███▊      | 75/196 [00:16<00:30,  3.93it/s] 39%|███▉      | 76/196 [00:17<00:29,  4.01it/s] 39%|███▉      | 77/196 [00:17<00:27,  4.35it/s] 40%|███▉      | 78/196 [00:17<00:28,  4.11it/s] 40%|████      | 79/196 [00:17<00:27,  4.21it/s] 41%|████      | 80/196 [00:17<00:23,  4.87it/s] 41%|████▏     | 81/196 [00:18<00:21,  5.43it/s] 42%|████▏     | 82/196 [00:18<00:23,  4.93it/s] 42%|████▏     | 83/196 [00:18<00:22,  5.07it/s] 43%|████▎     | 84/196 [00:18<00:27,  4.12it/s] 43%|████▎     | 85/196 [00:19<00:31,  3.49it/s] 44%|████▍     | 86/196 [00:19<00:32,  3.38it/s] 44%|████▍     | 87/196 [00:19<00:28,  3.80it/s] 45%|████▍     | 88/196 [00:19<00:23,  4.51it/s] 45%|████▌     | 89/196 [00:20<00:20,  5.24it/s] 46%|████▌     | 90/196 [00:20<00:26,  3.99it/s] 46%|████▋     | 91/196 [00:20<00:31,  3.29it/s] 47%|████▋     | 92/196 [00:21<00:33,  3.06it/s] 47%|████▋     | 93/196 [00:21<00:30,  3.43it/s] 48%|████▊     | 94/196 [00:21<00:24,  4.17it/s] 48%|████▊     | 95/196 [00:21<00:20,  4.97it/s] 49%|████▉     | 96/196 [00:21<00:17,  5.70it/s] 49%|████▉     | 97/196 [00:21<00:15,  6.24it/s] 50%|█████     | 98/196 [00:21<00:14,  6.87it/s] 51%|█████     | 99/196 [00:22<00:13,  7.19it/s] 51%|█████     | 100/196 [00:22<00:13,  7.33it/s] 52%|█████▏    | 101/196 [00:22<00:12,  7.77it/s] 52%|█████▏    | 102/196 [00:22<00:11,  8.01it/s] 53%|█████▎    | 103/196 [00:22<00:11,  8.24it/s] 53%|█████▎    | 104/196 [00:22<00:10,  8.49it/s] 54%|█████▎    | 105/196 [00:23<00:17,  5.35it/s] 54%|█████▍    | 106/196 [00:23<00:18,  4.78it/s] 55%|█████▍    | 107/196 [00:23<00:19,  4.53it/s] 55%|█████▌    | 108/196 [00:23<00:22,  3.97it/s] 56%|█████▌    | 109/196 [00:24<00:20,  4.20it/s] 56%|█████▌    | 110/196 [00:24<00:23,  3.65it/s] 57%|█████▋    | 111/196 [00:24<00:22,  3.70it/s] 57%|█████▋    | 112/196 [00:25<00:25,  3.33it/s] 58%|█████▊    | 113/196 [00:25<00:27,  3.07it/s] 58%|█████▊    | 114/196 [00:25<00:26,  3.13it/s] 59%|█████▊    | 115/196 [00:25<00:22,  3.53it/s] 59%|█████▉    | 116/196 [00:26<00:18,  4.23it/s] 60%|█████▉    | 117/196 [00:26<00:19,  3.99it/s] 60%|██████    | 118/196 [00:26<00:18,  4.19it/s] 61%|██████    | 119/196 [00:26<00:21,  3.52it/s] 61%|██████    | 120/196 [00:27<00:21,  3.59it/s] 62%|██████▏   | 121/196 [00:27<00:19,  3.80it/s] 62%|██████▏   | 122/196 [00:27<00:21,  3.40it/s] 63%|██████▎   | 123/196 [00:28<00:20,  3.62it/s] 63%|██████▎   | 124/196 [00:28<00:16,  4.32it/s] 64%|██████▍   | 125/196 [00:28<00:13,  5.11it/s] 64%|██████▍   | 126/196 [00:28<00:12,  5.75it/s] 65%|██████▍   | 127/196 [00:28<00:13,  5.13it/s] 65%|██████▌   | 128/196 [00:28<00:15,  4.39it/s] 66%|██████▌   | 129/196 [00:29<00:17,  3.82it/s] 66%|██████▋   | 130/196 [00:29<00:16,  4.03it/s] 67%|██████▋   | 131/196 [00:29<00:13,  4.83it/s] 67%|██████▋   | 132/196 [00:29<00:11,  5.68it/s] 68%|██████▊   | 133/196 [00:29<00:09,  6.32it/s] 68%|██████▊   | 134/196 [00:29<00:09,  6.70it/s] 69%|██████▉   | 135/196 [00:30<00:11,  5.20it/s] 69%|██████▉   | 136/196 [00:30<00:14,  4.27it/s] 70%|██████▉   | 137/196 [00:30<00:12,  4.55it/s] 70%|███████   | 138/196 [00:30<00:11,  5.22it/s] 71%|███████   | 139/196 [00:31<00:12,  4.40it/s] 71%|███████▏  | 140/196 [00:31<00:12,  4.43it/s] 72%|███████▏  | 141/196 [00:31<00:12,  4.37it/s] 72%|███████▏  | 142/196 [00:31<00:11,  4.57it/s] 73%|███████▎  | 143/196 [00:32<00:11,  4.55it/s] 73%|███████▎  | 144/196 [00:32<00:10,  4.95it/s] 74%|███████▍  | 145/196 [00:32<00:10,  4.73it/s] 74%|███████▍  | 146/196 [00:32<00:10,  4.90it/s] 75%|███████▌  | 147/196 [00:32<00:08,  5.52it/s] 76%|███████▌  | 148/196 [00:33<00:09,  5.25it/s] 76%|███████▌  | 149/196 [00:33<00:11,  4.21it/s] 77%|███████▋  | 150/196 [00:33<00:12,  3.68it/s] 77%|███████▋  | 151/196 [00:34<00:12,  3.58it/s] 78%|███████▊  | 152/196 [00:34<00:14,  3.04it/s] 78%|███████▊  | 153/196 [00:34<00:13,  3.22it/s] 79%|███████▊  | 154/196 [00:34<00:10,  3.95it/s] 79%|███████▉  | 155/196 [00:35<00:09,  4.32it/s] 80%|███████▉  | 156/196 [00:35<00:10,  3.89it/s] 80%|████████  | 157/196 [00:35<00:12,  3.12it/s] 81%|████████  | 158/196 [00:36<00:13,  2.84it/s] 81%|████████  | 159/196 [00:36<00:12,  2.93it/s] 82%|████████▏ | 160/196 [00:36<00:10,  3.43it/s] 82%|████████▏ | 161/196 [00:36<00:08,  4.18it/s] 83%|████████▎ | 162/196 [00:37<00:09,  3.66it/s] 83%|████████▎ | 163/196 [00:37<00:08,  3.73it/s] 84%|████████▎ | 164/196 [00:37<00:07,  4.42it/s] 84%|████████▍ | 165/196 [00:37<00:07,  4.08it/s] 85%|████████▍ | 166/196 [00:38<00:07,  4.20it/s] 85%|████████▌ | 167/196 [00:38<00:05,  4.86it/s] 86%|████████▌ | 168/196 [00:38<00:06,  4.59it/s] 86%|████████▌ | 169/196 [00:38<00:06,  4.08it/s] 87%|████████▋ | 170/196 [00:39<00:06,  3.78it/s] 87%|████████▋ | 171/196 [00:39<00:05,  4.22it/s] 88%|████████▊ | 172/196 [00:39<00:04,  5.07it/s] 88%|████████▊ | 173/196 [00:39<00:04,  5.73it/s] 89%|████████▉ | 174/196 [00:39<00:05,  4.19it/s] 89%|████████▉ | 175/196 [00:40<00:05,  3.51it/s] 90%|████████▉ | 176/196 [00:40<00:05,  3.86it/s] 90%|█████████ | 177/196 [00:40<00:04,  4.57it/s] 91%|█████████ | 178/196 [00:40<00:03,  5.15it/s] 91%|█████████▏| 179/196 [00:41<00:03,  4.28it/s] 92%|█████████▏| 180/196 [00:41<00:03,  4.33it/s] 92%|█████████▏| 181/196 [00:41<00:02,  5.09it/s] 93%|█████████▎| 182/196 [00:41<00:02,  5.62it/s] 93%|█████████▎| 183/196 [00:41<00:03,  4.29it/s] 94%|█████████▍| 184/196 [00:42<00:02,  4.16it/s] 94%|█████████▍| 185/196 [00:42<00:02,  4.25it/s] 95%|█████████▍| 186/196 [00:42<00:02,  4.53it/s] 95%|█████████▌| 187/196 [00:42<00:02,  3.68it/s] 96%|█████████▌| 188/196 [00:43<00:02,  3.19it/s] 96%|█████████▋| 189/196 [00:43<00:02,  2.85it/s] 97%|█████████▋| 190/196 [00:44<00:02,  2.68it/s] 97%|█████████▋| 191/196 [00:44<00:02,  2.44it/s] 98%|█████████▊| 192/196 [00:45<00:01,  2.47it/s] 98%|█████████▊| 193/196 [00:45<00:01,  2.93it/s] 99%|█████████▉| 194/196 [00:45<00:00,  3.24it/s] 99%|█████████▉| 195/196 [00:45<00:00,  3.27it/s]100%|██████████| 196/196 [00:46<00:00,  3.76it/s]100%|██████████| 196/196 [00:46<00:00,  4.25it/s]
[INFO|modelcard.py:449] 2024-12-14 12:14:32,244 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4489795918367347}]}
***** eval metrics *****
  epoch                   =     2.9949
  eval_accuracy           =      0.449
  eval_loss               =     0.6938
  eval_runtime            = 0:00:46.26
  eval_samples_per_second =      4.237
  eval_steps_per_second   =      4.237
usage: llamafactory-cli [-h] [--vllm_maxlen VLLM_MAXLEN]
                        [--vllm_gpu_util VLLM_GPU_UTIL]
                        [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]
                        [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
                        [--vllm_config VLLM_CONFIG] [--export_dir EXPORT_DIR]
                        [--export_size EXPORT_SIZE]
                        [--export_device {cpu,auto}]
                        [--export_quantization_bit EXPORT_QUANTIZATION_BIT]
                        [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
                        [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]
                        [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]
                        [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
                        [--export_hub_model_id EXPORT_HUB_MODEL_ID]
                        [--image_resolution IMAGE_RESOLUTION]
                        [--video_resolution VIDEO_RESOLUTION]
                        [--video_fps VIDEO_FPS] [--video_maxlen VIDEO_MAXLEN]
                        [--quantization_method {bitsandbytes,hqq,eetq}]
                        [--quantization_bit QUANTIZATION_BIT]
                        [--quantization_type {fp4,nf4}]
                        [--double_quantization [DOUBLE_QUANTIZATION]]
                        [--no_double_quantization]
                        [--quantization_device_map {auto}]
                        [--model_name_or_path MODEL_NAME_OR_PATH]
                        [--adapter_name_or_path ADAPTER_NAME_OR_PATH]
                        [--adapter_folder ADAPTER_FOLDER]
                        [--cache_dir CACHE_DIR]
                        [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
                        [--no_use_fast_tokenizer]
                        [--resize_vocab [RESIZE_VOCAB]]
                        [--split_special_tokens [SPLIT_SPECIAL_TOKENS]]
                        [--new_special_tokens NEW_SPECIAL_TOKENS]
                        [--model_revision MODEL_REVISION]
                        [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]
                        [--no_low_cpu_mem_usage]
                        [--rope_scaling {linear,dynamic}]
                        [--flash_attn {auto,disabled,sdpa,fa2}]
                        [--shift_attn [SHIFT_ATTN]]
                        [--mixture_of_depths {convert,load}]
                        [--use_unsloth [USE_UNSLOTH]]
                        [--use_unsloth_gc [USE_UNSLOTH_GC]]
                        [--enable_liger_kernel [ENABLE_LIGER_KERNEL]]
                        [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
                        [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]
                        [--upcast_layernorm [UPCAST_LAYERNORM]]
                        [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]
                        [--train_from_scratch [TRAIN_FROM_SCRATCH]]
                        [--infer_backend {huggingface,vllm}]
                        [--offload_folder OFFLOAD_FOLDER]
                        [--use_cache [USE_CACHE]] [--no_use_cache]
                        [--infer_dtype {auto,float16,bfloat16,float32}]
                        [--hf_hub_token HF_HUB_TOKEN]
                        [--ms_hub_token MS_HUB_TOKEN]
                        [--om_hub_token OM_HUB_TOKEN]
                        [--print_param_status [PRINT_PARAM_STATUS]]
                        [--template TEMPLATE] [--dataset DATASET]
                        [--eval_dataset EVAL_DATASET]
                        [--dataset_dir DATASET_DIR] [--image_dir IMAGE_DIR]
                        [--cutoff_len CUTOFF_LEN]
                        [--train_on_prompt [TRAIN_ON_PROMPT]]
                        [--mask_history [MASK_HISTORY]]
                        [--streaming [STREAMING]] [--buffer_size BUFFER_SIZE]
                        [--mix_strategy {concat,interleave_under,interleave_over}]
                        [--interleave_probs INTERLEAVE_PROBS]
                        [--overwrite_cache [OVERWRITE_CACHE]]
                        [--preprocessing_batch_size PREPROCESSING_BATCH_SIZE]
                        [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
                        [--max_samples MAX_SAMPLES]
                        [--eval_num_beams EVAL_NUM_BEAMS]
                        [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]
                        [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]
                        [--packing PACKING] [--neat_packing [NEAT_PACKING]]
                        [--tool_format TOOL_FORMAT]
                        [--tokenized_path TOKENIZED_PATH] --output_dir
                        OUTPUT_DIR
                        [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
                        [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
                        [--do_predict [DO_PREDICT]]
                        [--eval_strategy {no,steps,epoch}]
                        [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
                        [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
                        [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
                        [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                        [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                        [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                        [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
                        [--eval_delay EVAL_DELAY]
                        [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]
                        [--learning_rate LEARNING_RATE]
                        [--weight_decay WEIGHT_DECAY]
                        [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]
                        [--adam_epsilon ADAM_EPSILON]
                        [--max_grad_norm MAX_GRAD_NORM]
                        [--num_train_epochs NUM_TRAIN_EPOCHS]
                        [--max_steps MAX_STEPS]
                        [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]
                        [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]
                        [--warmup_ratio WARMUP_RATIO]
                        [--warmup_steps WARMUP_STEPS]
                        [--log_level {detail,debug,info,warning,error,critical,passive}]
                        [--log_level_replica {detail,debug,info,warning,error,critical,passive}]
                        [--log_on_each_node [LOG_ON_EACH_NODE]]
                        [--no_log_on_each_node] [--logging_dir LOGGING_DIR]
                        [--logging_strategy {no,steps,epoch}]
                        [--logging_first_step [LOGGING_FIRST_STEP]]
                        [--logging_steps LOGGING_STEPS]
                        [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]
                        [--no_logging_nan_inf_filter]
                        [--save_strategy {no,steps,epoch}]
                        [--save_steps SAVE_STEPS]
                        [--save_total_limit SAVE_TOTAL_LIMIT]
                        [--save_safetensors [SAVE_SAFETENSORS]]
                        [--no_save_safetensors]
                        [--save_on_each_node [SAVE_ON_EACH_NODE]]
                        [--save_only_model [SAVE_ONLY_MODEL]]
                        [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]
                        [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]
                        [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]
                        [--data_seed DATA_SEED]
                        [--jit_mode_eval [JIT_MODE_EVAL]]
                        [--use_ipex [USE_IPEX]] [--bf16 [BF16]]
                        [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]
                        [--half_precision_backend {auto,apex,cpu_amp}]
                        [--bf16_full_eval [BF16_FULL_EVAL]]
                        [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]
                        [--local_rank LOCAL_RANK]
                        [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]
                        [--tpu_num_cores TPU_NUM_CORES]
                        [--tpu_metrics_debug [TPU_METRICS_DEBUG]]
                        [--debug DEBUG [DEBUG ...]]
                        [--dataloader_drop_last [DATALOADER_DROP_LAST]]
                        [--eval_steps EVAL_STEPS]
                        [--dataloader_num_workers DATALOADER_NUM_WORKERS]
                        [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]
                        [--past_index PAST_INDEX] [--run_name RUN_NAME]
                        [--disable_tqdm DISABLE_TQDM]
                        [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
                        [--no_remove_unused_columns]
                        [--label_names LABEL_NAMES [LABEL_NAMES ...]]
                        [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
                        [--metric_for_best_model METRIC_FOR_BEST_MODEL]
                        [--greater_is_better GREATER_IS_BETTER]
                        [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]
                        [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]
                        [--fsdp_config FSDP_CONFIG]
                        [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]
                        [--accelerator_config ACCELERATOR_CONFIG]
                        [--deepspeed DEEPSPEED]
                        [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]
                        [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,adamw_torch_4bit,ademamix,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,ademamix_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_ademamix_32bit,paged_ademamix_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise,lomo,adalomo,grokadamw,schedule_free_adamw,schedule_free_sgd}]
                        [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]
                        [--group_by_length [GROUP_BY_LENGTH]]
                        [--length_column_name LENGTH_COLUMN_NAME]
                        [--report_to REPORT_TO]
                        [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]
                        [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]
                        [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]
                        [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]
                        [--no_dataloader_pin_memory]
                        [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]
                        [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
                        [--no_skip_memory_metrics]
                        [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]
                        [--push_to_hub [PUSH_TO_HUB]]
                        [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                        [--hub_model_id HUB_MODEL_ID]
                        [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]
                        [--hub_token HUB_TOKEN]
                        [--hub_private_repo [HUB_PRIVATE_REPO]]
                        [--hub_always_push [HUB_ALWAYS_PUSH]]
                        [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
                        [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]
                        [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]
                        [--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]]
                        [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]
                        [--no_eval_do_concat_batches]
                        [--fp16_backend {auto,apex,cpu_amp}]
                        [--evaluation_strategy {no,steps,epoch}]
                        [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]
                        [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]
                        [--push_to_hub_token PUSH_TO_HUB_TOKEN]
                        [--mp_parameters MP_PARAMETERS]
                        [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]
                        [--full_determinism [FULL_DETERMINISM]]
                        [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]
                        [--ddp_timeout DDP_TIMEOUT]
                        [--torch_compile [TORCH_COMPILE]]
                        [--torch_compile_backend TORCH_COMPILE_BACKEND]
                        [--torch_compile_mode TORCH_COMPILE_MODE]
                        [--dispatch_batches DISPATCH_BATCHES]
                        [--split_batches SPLIT_BATCHES]
                        [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]
                        [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]
                        [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]
                        [--optim_target_modules OPTIM_TARGET_MODULES]
                        [--batch_eval_metrics [BATCH_EVAL_METRICS]]
                        [--eval_on_start [EVAL_ON_START]]
                        [--use_liger_kernel [USE_LIGER_KERNEL]]
                        [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]
                        [--sortish_sampler [SORTISH_SAMPLER]]
                        [--predict_with_generate [PREDICT_WITH_GENERATE]]
                        [--generation_max_length GENERATION_MAX_LENGTH]
                        [--generation_num_beams GENERATION_NUM_BEAMS]
                        [--generation_config GENERATION_CONFIG]
                        [--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]
                        [--badam_start_block BADAM_START_BLOCK]
                        [--badam_switch_mode {ascending,descending,random,fixed}]
                        [--badam_switch_interval BADAM_SWITCH_INTERVAL]
                        [--badam_update_ratio BADAM_UPDATE_RATIO]
                        [--badam_mask_mode {adjacent,scatter}]
                        [--badam_verbose BADAM_VERBOSE]
                        [--use_galore [USE_GALORE]]
                        [--galore_target GALORE_TARGET]
                        [--galore_rank GALORE_RANK]
                        [--galore_update_interval GALORE_UPDATE_INTERVAL]
                        [--galore_scale GALORE_SCALE]
                        [--galore_proj_type {std,reverse_std,right,left,full}]
                        [--galore_layerwise [GALORE_LAYERWISE]]
                        [--pref_beta PREF_BETA] [--pref_ftx PREF_FTX]
                        [--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]
                        [--dpo_label_smoothing DPO_LABEL_SMOOTHING]
                        [--kto_chosen_weight KTO_CHOSEN_WEIGHT]
                        [--kto_rejected_weight KTO_REJECTED_WEIGHT]
                        [--simpo_gamma SIMPO_GAMMA]
                        [--ppo_buffer_size PPO_BUFFER_SIZE]
                        [--ppo_epochs PPO_EPOCHS]
                        [--ppo_score_norm [PPO_SCORE_NORM]]
                        [--ppo_target PPO_TARGET]
                        [--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]
                        [--ref_model REF_MODEL]
                        [--ref_model_adapters REF_MODEL_ADAPTERS]
                        [--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]
                        [--reward_model REWARD_MODEL]
                        [--reward_model_adapters REWARD_MODEL_ADAPTERS]
                        [--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]
                        [--reward_model_type {lora,full,api}]
                        [--additional_target ADDITIONAL_TARGET]
                        [--lora_alpha LORA_ALPHA]
                        [--lora_dropout LORA_DROPOUT] [--lora_rank LORA_RANK]
                        [--lora_target LORA_TARGET]
                        [--loraplus_lr_ratio LORAPLUS_LR_RATIO]
                        [--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]
                        [--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]
                        [--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]
                        [--pissa_convert [PISSA_CONVERT]]
                        [--create_new_adapter [CREATE_NEW_ADAPTER]]
                        [--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]
                        [--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]
                        [--freeze_extra_modules FREEZE_EXTRA_MODULES]
                        [--pure_bf16 [PURE_BF16]]
                        [--stage {pt,sft,rm,ppo,dpo,kto}]
                        [--finetuning_type {lora,freeze,full}]
                        [--use_llama_pro [USE_LLAMA_PRO]]
                        [--use_adam_mini [USE_ADAM_MINI]]
                        [--freeze_vision_tower [FREEZE_VISION_TOWER]]
                        [--no_freeze_vision_tower]
                        [--train_mm_proj_only [TRAIN_MM_PROJ_ONLY]]
                        [--compute_accuracy [COMPUTE_ACCURACY]]
                        [--plot_loss [PLOT_LOSS]]
                        [--include_effective_tokens_per_second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND]]
                        [--do_sample [DO_SAMPLE]] [--no_do_sample]
                        [--temperature TEMPERATURE] [--top_p TOP_P]
                        [--top_k TOP_K] [--num_beams NUM_BEAMS]
                        [--max_length MAX_LENGTH]
                        [--max_new_tokens MAX_NEW_TOKENS]
                        [--repetition_penalty REPETITION_PENALTY]
                        [--length_penalty LENGTH_PENALTY]
                        [--default_system DEFAULT_SYSTEM]
llamafactory-cli: error: the following arguments are required: --output_dir/--output-dir
usage: llamafactory-cli [-h] [--vllm_maxlen VLLM_MAXLEN]
                        [--vllm_gpu_util VLLM_GPU_UTIL]
                        [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]
                        [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
                        [--vllm_config VLLM_CONFIG] [--export_dir EXPORT_DIR]
                        [--export_size EXPORT_SIZE]
                        [--export_device {cpu,auto}]
                        [--export_quantization_bit EXPORT_QUANTIZATION_BIT]
                        [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
                        [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]
                        [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]
                        [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
                        [--export_hub_model_id EXPORT_HUB_MODEL_ID]
                        [--image_resolution IMAGE_RESOLUTION]
                        [--video_resolution VIDEO_RESOLUTION]
                        [--video_fps VIDEO_FPS] [--video_maxlen VIDEO_MAXLEN]
                        [--quantization_method {bitsandbytes,hqq,eetq}]
                        [--quantization_bit QUANTIZATION_BIT]
                        [--quantization_type {fp4,nf4}]
                        [--double_quantization [DOUBLE_QUANTIZATION]]
                        [--no_double_quantization]
                        [--quantization_device_map {auto}]
                        [--model_name_or_path MODEL_NAME_OR_PATH]
                        [--adapter_name_or_path ADAPTER_NAME_OR_PATH]
                        [--adapter_folder ADAPTER_FOLDER]
                        [--cache_dir CACHE_DIR]
                        [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
                        [--no_use_fast_tokenizer]
                        [--resize_vocab [RESIZE_VOCAB]]
                        [--split_special_tokens [SPLIT_SPECIAL_TOKENS]]
                        [--new_special_tokens NEW_SPECIAL_TOKENS]
                        [--model_revision MODEL_REVISION]
                        [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]
                        [--no_low_cpu_mem_usage]
                        [--rope_scaling {linear,dynamic}]
                        [--flash_attn {auto,disabled,sdpa,fa2}]
                        [--shift_attn [SHIFT_ATTN]]
                        [--mixture_of_depths {convert,load}]
                        [--use_unsloth [USE_UNSLOTH]]
                        [--use_unsloth_gc [USE_UNSLOTH_GC]]
                        [--enable_liger_kernel [ENABLE_LIGER_KERNEL]]
                        [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
                        [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]
                        [--upcast_layernorm [UPCAST_LAYERNORM]]
                        [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]
                        [--train_from_scratch [TRAIN_FROM_SCRATCH]]
                        [--infer_backend {huggingface,vllm}]
                        [--offload_folder OFFLOAD_FOLDER]
                        [--use_cache [USE_CACHE]] [--no_use_cache]
                        [--infer_dtype {auto,float16,bfloat16,float32}]
                        [--hf_hub_token HF_HUB_TOKEN]
                        [--ms_hub_token MS_HUB_TOKEN]
                        [--om_hub_token OM_HUB_TOKEN]
                        [--print_param_status [PRINT_PARAM_STATUS]]
                        [--template TEMPLATE] [--dataset DATASET]
                        [--eval_dataset EVAL_DATASET]
                        [--dataset_dir DATASET_DIR] [--image_dir IMAGE_DIR]
                        [--cutoff_len CUTOFF_LEN]
                        [--train_on_prompt [TRAIN_ON_PROMPT]]
                        [--mask_history [MASK_HISTORY]]
                        [--streaming [STREAMING]] [--buffer_size BUFFER_SIZE]
                        [--mix_strategy {concat,interleave_under,interleave_over}]
                        [--interleave_probs INTERLEAVE_PROBS]
                        [--overwrite_cache [OVERWRITE_CACHE]]
                        [--preprocessing_batch_size PREPROCESSING_BATCH_SIZE]
                        [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
                        [--max_samples MAX_SAMPLES]
                        [--eval_num_beams EVAL_NUM_BEAMS]
                        [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]
                        [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]
                        [--packing PACKING] [--neat_packing [NEAT_PACKING]]
                        [--tool_format TOOL_FORMAT]
                        [--tokenized_path TOKENIZED_PATH] --output_dir
                        OUTPUT_DIR
                        [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]
                        [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]
                        [--do_predict [DO_PREDICT]]
                        [--eval_strategy {no,steps,epoch}]
                        [--prediction_loss_only [PREDICTION_LOSS_ONLY]]
                        [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]
                        [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]
                        [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]
                        [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]
                        [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                        [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]
                        [--eval_delay EVAL_DELAY]
                        [--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]
                        [--learning_rate LEARNING_RATE]
                        [--weight_decay WEIGHT_DECAY]
                        [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]
                        [--adam_epsilon ADAM_EPSILON]
                        [--max_grad_norm MAX_GRAD_NORM]
                        [--num_train_epochs NUM_TRAIN_EPOCHS]
                        [--max_steps MAX_STEPS]
                        [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,warmup_stable_decay}]
                        [--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]
                        [--warmup_ratio WARMUP_RATIO]
                        [--warmup_steps WARMUP_STEPS]
                        [--log_level {detail,debug,info,warning,error,critical,passive}]
                        [--log_level_replica {detail,debug,info,warning,error,critical,passive}]
                        [--log_on_each_node [LOG_ON_EACH_NODE]]
                        [--no_log_on_each_node] [--logging_dir LOGGING_DIR]
                        [--logging_strategy {no,steps,epoch}]
                        [--logging_first_step [LOGGING_FIRST_STEP]]
                        [--logging_steps LOGGING_STEPS]
                        [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]
                        [--no_logging_nan_inf_filter]
                        [--save_strategy {no,steps,epoch}]
                        [--save_steps SAVE_STEPS]
                        [--save_total_limit SAVE_TOTAL_LIMIT]
                        [--save_safetensors [SAVE_SAFETENSORS]]
                        [--no_save_safetensors]
                        [--save_on_each_node [SAVE_ON_EACH_NODE]]
                        [--save_only_model [SAVE_ONLY_MODEL]]
                        [--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]
                        [--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]
                        [--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]
                        [--data_seed DATA_SEED]
                        [--jit_mode_eval [JIT_MODE_EVAL]]
                        [--use_ipex [USE_IPEX]] [--bf16 [BF16]]
                        [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]
                        [--half_precision_backend {auto,apex,cpu_amp}]
                        [--bf16_full_eval [BF16_FULL_EVAL]]
                        [--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]
                        [--local_rank LOCAL_RANK]
                        [--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]
                        [--tpu_num_cores TPU_NUM_CORES]
                        [--tpu_metrics_debug [TPU_METRICS_DEBUG]]
                        [--debug DEBUG [DEBUG ...]]
                        [--dataloader_drop_last [DATALOADER_DROP_LAST]]
                        [--eval_steps EVAL_STEPS]
                        [--dataloader_num_workers DATALOADER_NUM_WORKERS]
                        [--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]
                        [--past_index PAST_INDEX] [--run_name RUN_NAME]
                        [--disable_tqdm DISABLE_TQDM]
                        [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]
                        [--no_remove_unused_columns]
                        [--label_names LABEL_NAMES [LABEL_NAMES ...]]
                        [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]
                        [--metric_for_best_model METRIC_FOR_BEST_MODEL]
                        [--greater_is_better GREATER_IS_BETTER]
                        [--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]
                        [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]
                        [--fsdp_config FSDP_CONFIG]
                        [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]
                        [--accelerator_config ACCELERATOR_CONFIG]
                        [--deepspeed DEEPSPEED]
                        [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]
                        [--optim {adamw_hf,adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,adamw_torch_4bit,ademamix,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,ademamix_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_ademamix_32bit,paged_ademamix_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise,lomo,adalomo,grokadamw,schedule_free_adamw,schedule_free_sgd}]
                        [--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]
                        [--group_by_length [GROUP_BY_LENGTH]]
                        [--length_column_name LENGTH_COLUMN_NAME]
                        [--report_to REPORT_TO]
                        [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]
                        [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]
                        [--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]
                        [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]
                        [--no_dataloader_pin_memory]
                        [--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]
                        [--skip_memory_metrics [SKIP_MEMORY_METRICS]]
                        [--no_skip_memory_metrics]
                        [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]
                        [--push_to_hub [PUSH_TO_HUB]]
                        [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                        [--hub_model_id HUB_MODEL_ID]
                        [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]
                        [--hub_token HUB_TOKEN]
                        [--hub_private_repo [HUB_PRIVATE_REPO]]
                        [--hub_always_push [HUB_ALWAYS_PUSH]]
                        [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
                        [--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]
                        [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]
                        [--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]]
                        [--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]
                        [--no_eval_do_concat_batches]
                        [--fp16_backend {auto,apex,cpu_amp}]
                        [--evaluation_strategy {no,steps,epoch}]
                        [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]
                        [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]
                        [--push_to_hub_token PUSH_TO_HUB_TOKEN]
                        [--mp_parameters MP_PARAMETERS]
                        [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]
                        [--full_determinism [FULL_DETERMINISM]]
                        [--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]
                        [--ddp_timeout DDP_TIMEOUT]
                        [--torch_compile [TORCH_COMPILE]]
                        [--torch_compile_backend TORCH_COMPILE_BACKEND]
                        [--torch_compile_mode TORCH_COMPILE_MODE]
                        [--dispatch_batches DISPATCH_BATCHES]
                        [--split_batches SPLIT_BATCHES]
                        [--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]
                        [--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]
                        [--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]
                        [--optim_target_modules OPTIM_TARGET_MODULES]
                        [--batch_eval_metrics [BATCH_EVAL_METRICS]]
                        [--eval_on_start [EVAL_ON_START]]
                        [--use_liger_kernel [USE_LIGER_KERNEL]]
                        [--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]
                        [--sortish_sampler [SORTISH_SAMPLER]]
                        [--predict_with_generate [PREDICT_WITH_GENERATE]]
                        [--generation_max_length GENERATION_MAX_LENGTH]
                        [--generation_num_beams GENERATION_NUM_BEAMS]
                        [--generation_config GENERATION_CONFIG]
                        [--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]
                        [--badam_start_block BADAM_START_BLOCK]
                        [--badam_switch_mode {ascending,descending,random,fixed}]
                        [--badam_switch_interval BADAM_SWITCH_INTERVAL]
                        [--badam_update_ratio BADAM_UPDATE_RATIO]
                        [--badam_mask_mode {adjacent,scatter}]
                        [--badam_verbose BADAM_VERBOSE]
                        [--use_galore [USE_GALORE]]
                        [--galore_target GALORE_TARGET]
                        [--galore_rank GALORE_RANK]
                        [--galore_update_interval GALORE_UPDATE_INTERVAL]
                        [--galore_scale GALORE_SCALE]
                        [--galore_proj_type {std,reverse_std,right,left,full}]
                        [--galore_layerwise [GALORE_LAYERWISE]]
                        [--pref_beta PREF_BETA] [--pref_ftx PREF_FTX]
                        [--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]
                        [--dpo_label_smoothing DPO_LABEL_SMOOTHING]
                        [--kto_chosen_weight KTO_CHOSEN_WEIGHT]
                        [--kto_rejected_weight KTO_REJECTED_WEIGHT]
                        [--simpo_gamma SIMPO_GAMMA]
                        [--ppo_buffer_size PPO_BUFFER_SIZE]
                        [--ppo_epochs PPO_EPOCHS]
                        [--ppo_score_norm [PPO_SCORE_NORM]]
                        [--ppo_target PPO_TARGET]
                        [--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]
                        [--ref_model REF_MODEL]
                        [--ref_model_adapters REF_MODEL_ADAPTERS]
                        [--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]
                        [--reward_model REWARD_MODEL]
                        [--reward_model_adapters REWARD_MODEL_ADAPTERS]
                        [--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]
                        [--reward_model_type {lora,full,api}]
                        [--additional_target ADDITIONAL_TARGET]
                        [--lora_alpha LORA_ALPHA]
                        [--lora_dropout LORA_DROPOUT] [--lora_rank LORA_RANK]
                        [--lora_target LORA_TARGET]
                        [--loraplus_lr_ratio LORAPLUS_LR_RATIO]
                        [--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]
                        [--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]
                        [--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]
                        [--pissa_convert [PISSA_CONVERT]]
                        [--create_new_adapter [CREATE_NEW_ADAPTER]]
                        [--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]
                        [--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]
                        [--freeze_extra_modules FREEZE_EXTRA_MODULES]
                        [--pure_bf16 [PURE_BF16]]
                        [--stage {pt,sft,rm,ppo,dpo,kto}]
                        [--finetuning_type {lora,freeze,full}]
                        [--use_llama_pro [USE_LLAMA_PRO]]
                        [--use_adam_mini [USE_ADAM_MINI]]
                        [--freeze_vision_tower [FREEZE_VISION_TOWER]]
                        [--no_freeze_vision_tower]
                        [--train_mm_proj_only [TRAIN_MM_PROJ_ONLY]]
                        [--compute_accuracy [COMPUTE_ACCURACY]]
                        [--plot_loss [PLOT_LOSS]]
                        [--include_effective_tokens_per_second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND]]
                        [--do_sample [DO_SAMPLE]] [--no_do_sample]
                        [--temperature TEMPERATURE] [--top_p TOP_P]
                        [--top_k TOP_K] [--num_beams NUM_BEAMS]
                        [--max_length MAX_LENGTH]
                        [--max_new_tokens MAX_NEW_TOKENS]
                        [--repetition_penalty REPETITION_PENALTY]
                        [--length_penalty LENGTH_PENALTY]
                        [--default_system DEFAULT_SYSTEM]
llamafactory-cli: error: the following arguments are required: --output_dir/--output-dir
